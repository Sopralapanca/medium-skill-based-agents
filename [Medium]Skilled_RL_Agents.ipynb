{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0OpfDo6BptY"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIS7A-97w03Y",
        "outputId": "57a0547e-26a8-471d-a917-332aa8868d9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'medium-skill-based-agents'...\n",
            "remote: Enumerating objects: 17466, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 17466 (delta 8), reused 20 (delta 7), pack-reused 17439 (from 2)\u001b[K\n",
            "Receiving objects: 100% (17466/17466), 153.10 MiB | 11.17 MiB/s, done.\n",
            "Resolving deltas: 100% (7866/7866), done.\n",
            "Updating files: 100% (33/33), done.\n",
            "/content/medium-skill-based-agents\n",
            " agents\t\t\t\t    pyproject.toml     train_autoenc.py\n",
            " configs.yaml\t\t\t    README.md\t       train_usr.py\n",
            " convert_to_pt.py\t\t    skills\t       train_vok.py\n",
            " create_dataset.py\t\t    test_packages.py   train_vos.py\n",
            " environment_configs\t\t    test_train.py      utils\n",
            " MANIFEST.in\t\t\t    test_vok.ipynb     uv.lock\n",
            "'[Medium]Skilled_RL_Agents.ipynb'   test_vos.ipynb\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/Sopralapanca/medium-skill-based-agents.git\n",
        "# %cd /content/medium-skill-based-agents\n",
        "# !ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UREODkjw6ZL",
        "outputId": "956bf22f-297e-4f4f-c6ac-a695484b050e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "uv 0.9.17\n"
          ]
        }
      ],
      "source": [
        "!uv --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3VGFLkD12DN"
      },
      "outputs": [],
      "source": [
        "# ! yes | pip uninstall gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkFEB459xstf",
        "outputId": "9b872278-6cb9-4c2c-96b8-80d8d98e25bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m148 packages\u001b[0m \u001b[2min 7.55s\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m45 packages\u001b[0m \u001b[2min 1m 19s\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m11 packages\u001b[0m \u001b[2min 1.98s\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m45 packages\u001b[0m \u001b[2min 990ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1matariari\u001b[0m\u001b[2m==0.0.1 (from git+https://github.com/Sopralapanca/atari-representation-learning.git@9cc4c53fa44f45c14061343b57e8a40e25945028)\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcolorlog\u001b[0m\u001b[2m==6.10.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcomm\u001b[0m\u001b[2m==0.2.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdataproperty\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mgym\u001b[0m\u001b[2m==0.25.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgym\u001b[0m\u001b[2m==0.26.2\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mgymnasium\u001b[0m\u001b[2m==1.2.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgymnasium\u001b[0m\u001b[2m==1.2.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhuggingface-sb3\u001b[0m\u001b[2m==3.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mipykernel\u001b[0m\u001b[2m==6.17.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mipykernel\u001b[0m\u001b[2m==7.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjedi\u001b[0m\u001b[2m==0.19.2\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mjupyter-client\u001b[0m\u001b[2m==7.4.9\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjupyter-client\u001b[0m\u001b[2m==8.7.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mkornia\u001b[0m\u001b[2m==0.8.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mkornia-rs\u001b[0m\u001b[2m==0.1.10\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmbstrdecoder\u001b[0m\u001b[2m==1.1.4\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mmoviepy\u001b[0m\u001b[2m==1.0.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmoviepy\u001b[0m\u001b[2m==2.2.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.8.4.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.8.93\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.10.2.21\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.3.3.83\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufile-cu12\u001b[0m\u001b[2m==1.13.1.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.9.90\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.7.3.90\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.8.93\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.7.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.28.9\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.27.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.8.93\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvshmem-cu12\u001b[0m\u001b[2m==3.3.20\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1moptuna\u001b[0m\u001b[2m==4.6.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpathvalidate\u001b[0m\u001b[2m==3.3.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpytablewriter\u001b[0m\u001b[2m==1.2.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrl-zoo3\u001b[0m\u001b[2m==2.7.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msb3-contrib\u001b[0m\u001b[2m==2.7.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mscikit-learn\u001b[0m\u001b[2m==1.6.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mscikit-learn\u001b[0m\u001b[2m==1.8.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mshimmy\u001b[0m\u001b[2m==2.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mstable-baselines3\u001b[0m\u001b[2m==2.7.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtabledata\u001b[0m\u001b[2m==1.3.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtcolorpy\u001b[0m\u001b[2m==0.1.7\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtensorboard\u001b[0m\u001b[2m==2.19.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtensorboard\u001b[0m\u001b[2m==2.20.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtensorflow\u001b[0m\u001b[2m==2.19.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtensorflow\u001b[0m\u001b[2m==2.20.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.9.0+cpu\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.9.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.24.0+cpu\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.24.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.5.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtypepy\u001b[0m\u001b[2m==1.3.4\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# !uv pip install -r pyproject.toml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQ-Fqi6A2H5t"
      },
      "outputs": [],
      "source": [
        "# !python create_dataset.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PiyVRcgMUdK"
      },
      "outputs": [],
      "source": [
        "# !python train_vos.py\n",
        "# !python train_vok.py\n",
        "# !python train_usr.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_C88ROwMUdL"
      },
      "source": [
        "Import the required packages, build the environment and test if it works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1xZN1JaoBj4",
        "outputId": "19907a38-ba08-464a-9825-9b18e54c4154"
      },
      "outputs": [],
      "source": [
        "# general imports\n",
        "import torch\n",
        "import yaml\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "\n",
        "# training imports\n",
        "from stable_baselines3.common.env_util import make_atari_env\n",
        "from stable_baselines3.common.vec_env import VecFrameStack, VecTransposeImage\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# IMPORTANT - REGISTER THE ENVIRONMENTS\n",
        "import gymnasium as gym\n",
        "import ale_py\n",
        "gym.register_envs(ale_py)\n",
        "\n",
        "# Load config\n",
        "_config_path = \"./configs.yaml\"\n",
        "\n",
        "_config = {}\n",
        "with open(_config_path, \"r\") as f:\n",
        "    _config = yaml.safe_load(f) or {}\n",
        "\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  # ignore tensorflow warnings about CPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "seed = None\n",
        "if seed is not None:\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "\n",
        "#envs = _config.get(\"ENVS\", [\"PongNoFrameskip-v4\"])[0]\n",
        "env = \"PongNoFrameskip-v4\"\n",
        "with open(f'environment_configs/{env}.yaml', 'r') as file:\n",
        "        environment_configuration = yaml.safe_load(file)[\"config\"]\n",
        "\n",
        "\n",
        "environment_configuration[\"f_ext_kwargs\"][\"device\"] = device  #do not comment this, it is the parameter passed to the feature extractor\n",
        "environment_configuration[\"game\"] = env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KjcYdvrRWjXI"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "# key = os.getenv(\"WANDB_API_KEY\")\n",
        "# if key is None:\n",
        "#     raise ValueError(\"WANDB_API_KEY not set\")\n",
        "\n",
        "# wandb.login(key=key)\n",
        "\n",
        "# tags = [\n",
        "#     f\"fe:{environment_configuration['f_ext_name']}\",\n",
        "#     f\"game:{environment_configuration['game']}\",\n",
        "# ]\n",
        "\n",
        "# run = wandb.init(\n",
        "#     project=\"medium-skill-based-agents\",\n",
        "#     config=environment_configuration,\n",
        "#     sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
        "#     monitor_gym=False,  # auto-upload the videos of agents playing the game\n",
        "#     group=f\"{environment_configuration['game']}\",\n",
        "#     tags=tags\n",
        "#     # save_code = True,  # optional\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "wolJs-iNMUdL",
        "outputId": "380143db-de7f-4eb8-d65b-23f0f6b49e2d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# monitor_dir = f\"monitor/{run.id}\"\n",
        "monitor_dir = \"ppo\"\n",
        "\n",
        "vec_envs = make_atari_env(env, n_envs=environment_configuration[\"n_envs\"], seed=seed)\n",
        "vec_envs = VecFrameStack(vec_envs, n_stack=environment_configuration[\"n_stacks\"])\n",
        "vec_envs = VecTransposeImage(vec_envs)\n",
        "\n",
        "# execute some steps with random moves\n",
        "obs = vec_envs.reset()\n",
        "\n",
        "for i in range(10):\n",
        "    action = [vec_envs.action_space.sample() for _ in range(environment_configuration[\"n_envs\"])]\n",
        "    obs, rewards, dones, info = vec_envs.step(action)\n",
        "\n",
        "# obs[0] has shape (4, 84, 84) because there are 4 stacked environments, take the first\n",
        "observation = obs[0][-1]\n",
        "plt.imshow(observation)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_98s5PWMUdM",
        "outputId": "45789a98-3844-47b1-82fa-e3664cde07d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4, 84, 84)\n",
            "\n",
            "Testing skill input adapters:\n",
            "\n",
            "Skill: state_rep_uns\n",
            "Input shape: torch.Size([1, 4, 84, 84])\n",
            "After adapter: torch.Size([1, 1, 160, 210])\n",
            "\n",
            "Skill: obj_key_enc\n",
            "Input shape: torch.Size([1, 4, 84, 84])\n",
            "After adapter: torch.Size([1, 1, 84, 84])\n",
            "\n",
            "Skill: obj_key_key\n",
            "Input shape: torch.Size([1, 4, 84, 84])\n",
            "After adapter: torch.Size([1, 1, 84, 84])\n",
            "\n",
            "Skill: vid_obj_seg\n",
            "Input shape: torch.Size([1, 4, 84, 84])\n",
            "After adapter: torch.Size([1, 2, 84, 84])\n"
          ]
        }
      ],
      "source": [
        "from skills.autoencoder import Autoencoder\n",
        "from skills.unsupervised_state_representation import UnsupervisedStateRepresentationModel\n",
        "from skills.video_object_keypoints import Transporter\n",
        "from skills.video_object_segmentation import VideoObjectSegmentationModel\n",
        "\n",
        "# init skills\n",
        "autoencoder = Autoencoder(channels=1).to(device)\n",
        "\n",
        "# observation has shape (32, 4, 84, 84),\n",
        "# observation = obs[0][0][None, :, :] # (1, 84, 84)\n",
        "\n",
        "print(obs[0].shape) # (4, 84, 84)\n",
        "\n",
        "\n",
        "usr = UnsupervisedStateRepresentationModel(observation=obs[0], device=device)\n",
        "vok = Transporter().to(device)\n",
        "vos = VideoObjectSegmentationModel(device=device)\n",
        "\n",
        "\n",
        "skills = [\n",
        "    usr.get_skill(device=device),\n",
        "    vok.get_skill(device=device, keynet_or_encoder=\"encoder\"),\n",
        "    vok.get_skill(device=device, keynet_or_encoder=\"keynet\"),\n",
        "    vos.get_skill(device=device)\n",
        "]\n",
        "\n",
        "# Test each skill's input adapter\n",
        "print(\"\\nTesting skill input adapters:\")\n",
        "test_obs = obs[:1]  # Take one sample from batch\n",
        "test_obs = torch.tensor(test_obs, dtype=torch.float32).to(device)\n",
        "\n",
        "for skill in skills:\n",
        "    print(f\"\\nSkill: {skill.name}\")\n",
        "    print(f\"Input shape: {test_obs.shape}\")\n",
        "    adapted = skill.input_adapter(test_obs)\n",
        "    print(f\"After adapter: {adapted.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.feature_extractors import WeightSharingAttentionExtractor, MixtureOfExpertsExtractor\n",
        "\n",
        "f_ext_kwargs = environment_configuration[\"f_ext_kwargs\"]\n",
        "\n",
        "feature_extractor_class = \"moe\"\n",
        "\n",
        "if feature_extractor_class == \"wsa\":\n",
        "    environment_configuration[\"f_ext_name\"] = \"wsharing_attention_ext\"\n",
        "    environment_configuration[\"f_ext_class\"] = WeightSharingAttentionExtractor\n",
        "elif feature_extractor_class == \"moe\":\n",
        "    environment_configuration[\"f_ext_name\"] = \"moe_ext\"\n",
        "    environment_configuration[\"f_ext_class\"] = MixtureOfExpertsExtractor\n",
        "elif feature_extractor_class == \"ppo\":\n",
        "    pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zf5R1JfAMUdN",
        "outputId": "07c920c1-9617-4f4f-c894-4af58cc90a9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "from stable_baselines3 import PPO\n",
        "from rl_zoo3.utils import linear_schedule\n",
        "\n",
        "f_ext_kwargs[\"skills\"] = skills\n",
        "f_ext_kwargs[\"features_dim\"] = 256\n",
        "\n",
        "policy_kwargs = dict(\n",
        "    features_extractor_class=environment_configuration[\"f_ext_class\"] if \"f_ext_class\" in environment_configuration else None,\n",
        "    features_extractor_kwargs=f_ext_kwargs if \"f_ext_kwargs\" in environment_configuration else None,\n",
        "    net_arch={\n",
        "        \"pi\": environment_configuration[\"net_arch_pi\"],\n",
        "        \"vf\": environment_configuration[\"net_arch_vf\"],\n",
        "    },\n",
        "    # activation_fn=th.nn.ReLU,  # use ReLU in case of multiple layers for the policy learning network\n",
        ")\n",
        "\n",
        "logdir = \"./tensorboard_logs\"\n",
        "\n",
        "model = PPO(\n",
        "    \"CnnPolicy\",\n",
        "    vec_envs,\n",
        "    learning_rate=linear_schedule(environment_configuration[\"learning_rate\"]),\n",
        "    n_steps=128,\n",
        "    n_epochs=4,\n",
        "    batch_size=environment_configuration[\"batch_size\"],\n",
        "    clip_range=linear_schedule(environment_configuration[\"clip_range\"]),\n",
        "    normalize_advantage=environment_configuration[\"normalize\"],\n",
        "    ent_coef=environment_configuration[\"ent_coef\"],\n",
        "    vf_coef=environment_configuration[\"vf_coef\"],\n",
        "    policy_kwargs=policy_kwargs,\n",
        "    verbose=1,\n",
        "    device=device,\n",
        "    tensorboard_log=logdir,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "id": "mQeF5UxuMUdO",
        "outputId": "7f6a84f7-6199-4887-abfd-3fced2aa0946"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logging to ./tensorboard_logs/PPO_2\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 89   |\n",
            "|    iterations      | 1    |\n",
            "|    time_elapsed    | 11   |\n",
            "|    total_timesteps | 1024 |\n",
            "-----------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 23           |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 86           |\n",
            "|    total_timesteps      | 2048         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 4.553469e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.0898       |\n",
            "|    entropy_loss         | -1.79        |\n",
            "|    explained_variance   | -0.0877      |\n",
            "|    learning_rate        | 0.000224     |\n",
            "|    loss                 | 0.03         |\n",
            "|    n_updates            | 4            |\n",
            "|    policy_gradient_loss | -4.41e-05    |\n",
            "|    value_loss           | 0.128        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 19           |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 160          |\n",
            "|    total_timesteps      | 3072         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0001125671 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.0795       |\n",
            "|    entropy_loss         | -1.79        |\n",
            "|    explained_variance   | 0.000196     |\n",
            "|    learning_rate        | 0.000199     |\n",
            "|    loss                 | 0.0466       |\n",
            "|    n_updates            | 8            |\n",
            "|    policy_gradient_loss | -0.000212    |\n",
            "|    value_loss           | 0.164        |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 17            |\n",
            "|    iterations           | 4             |\n",
            "|    time_elapsed         | 236           |\n",
            "|    total_timesteps      | 4096          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 9.4806775e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.0693        |\n",
            "|    entropy_loss         | -1.79         |\n",
            "|    explained_variance   | 9.85e-05      |\n",
            "|    learning_rate        | 0.000173      |\n",
            "|    loss                 | 0.0354        |\n",
            "|    n_updates            | 12            |\n",
            "|    policy_gradient_loss | 0.0002        |\n",
            "|    value_loss           | 0.124         |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 16           |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 308          |\n",
            "|    total_timesteps      | 5120         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 6.519939e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.059        |\n",
            "|    entropy_loss         | -1.79        |\n",
            "|    explained_variance   | 5.53e-05     |\n",
            "|    learning_rate        | 0.000148     |\n",
            "|    loss                 | 0.0697       |\n",
            "|    n_updates            | 16           |\n",
            "|    policy_gradient_loss | 5.99e-05     |\n",
            "|    value_loss           | 0.157        |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 16            |\n",
            "|    iterations           | 6             |\n",
            "|    time_elapsed         | 382           |\n",
            "|    total_timesteps      | 6144          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00013596588 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.0488        |\n",
            "|    entropy_loss         | -1.79         |\n",
            "|    explained_variance   | 7.21e-06      |\n",
            "|    learning_rate        | 0.000122      |\n",
            "|    loss                 | 0.0557        |\n",
            "|    n_updates            | 20            |\n",
            "|    policy_gradient_loss | -0.000559     |\n",
            "|    value_loss           | 0.14          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 3.44e+03      |\n",
            "|    ep_rew_mean          | -20.6         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 15            |\n",
            "|    iterations           | 7             |\n",
            "|    time_elapsed         | 458           |\n",
            "|    total_timesteps      | 7168          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00013958535 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.0386        |\n",
            "|    entropy_loss         | -1.79         |\n",
            "|    explained_variance   | 9.46e-05      |\n",
            "|    learning_rate        | 9.64e-05      |\n",
            "|    loss                 | 0.035         |\n",
            "|    n_updates            | 24            |\n",
            "|    policy_gradient_loss | 0.000481      |\n",
            "|    value_loss           | 0.111         |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 3.6e+03      |\n",
            "|    ep_rew_mean          | -20.3        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 15           |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 533          |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 3.567437e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.0283       |\n",
            "|    entropy_loss         | -1.79        |\n",
            "|    explained_variance   | 0.000109     |\n",
            "|    learning_rate        | 7.08e-05     |\n",
            "|    loss                 | 0.0336       |\n",
            "|    n_updates            | 28           |\n",
            "|    policy_gradient_loss | -0.000259    |\n",
            "|    value_loss           | 0.103        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 3.6e+03      |\n",
            "|    ep_rew_mean          | -20.3        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 15           |\n",
            "|    iterations           | 9            |\n",
            "|    time_elapsed         | 612          |\n",
            "|    total_timesteps      | 9216         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 4.598638e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.0181       |\n",
            "|    entropy_loss         | -1.79        |\n",
            "|    explained_variance   | 7.35e-05     |\n",
            "|    learning_rate        | 4.52e-05     |\n",
            "|    loss                 | 0.0277       |\n",
            "|    n_updates            | 32           |\n",
            "|    policy_gradient_loss | -0.000185    |\n",
            "|    value_loss           | 0.104        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 3.6e+03      |\n",
            "|    ep_rew_mean          | -20.3        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 14           |\n",
            "|    iterations           | 10           |\n",
            "|    time_elapsed         | 688          |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.720788e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.00784      |\n",
            "|    entropy_loss         | -1.79        |\n",
            "|    explained_variance   | 7.53e-05     |\n",
            "|    learning_rate        | 1.96e-05     |\n",
            "|    loss                 | 0.0457       |\n",
            "|    n_updates            | 36           |\n",
            "|    policy_gradient_loss | 1.2e-05      |\n",
            "|    value_loss           | 0.136        |\n",
            "------------------------------------------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<stable_baselines3.ppo.ppo.PPO at 0x7fae38e3f170>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold, StopTrainingOnNoModelImprovement\n",
        "from wandb.integration.sb3 import WandbCallback\n",
        "\n",
        "eval_env = make_atari_env(env, n_envs=environment_configuration[\"n_envs\"])\n",
        "eval_env = VecFrameStack(eval_env, n_stack=environment_configuration[\"n_stacks\"])\n",
        "eval_env = VecTransposeImage(eval_env)\n",
        "\n",
        "eval_logs = f\"eval_logs/{env}/{monitor_dir}\"\n",
        "os.makedirs(eval_logs, exist_ok=True)\n",
        "\n",
        "eval_callback = EvalCallback(\n",
        "    eval_env,\n",
        "    n_eval_episodes=100,\n",
        "    best_model_save_path=f\"./agents/{monitor_dir}\",\n",
        "    log_path=eval_logs,\n",
        "    eval_freq=5000 * environment_configuration[\"n_envs\"],\n",
        "    verbose=0,\n",
        ")\n",
        "\n",
        "callbacks = [\n",
        "    #WandbCallback(verbose=0),\n",
        "    eval_callback\n",
        "]\n",
        "\n",
        "model.learn(10000, callback=callbacks) #tb_log_name=run.id)\n",
        "#run.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCOXVdoMMUdO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "medium-skill-based-agents",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
