{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0OpfDo6BptY"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIS7A-97w03Y",
        "outputId": "57a0547e-26a8-471d-a917-332aa8868d9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'medium-skill-based-agents'...\n",
            "remote: Enumerating objects: 17466, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 17466 (delta 8), reused 20 (delta 7), pack-reused 17439 (from 2)\u001b[K\n",
            "Receiving objects: 100% (17466/17466), 153.10 MiB | 11.17 MiB/s, done.\n",
            "Resolving deltas: 100% (7866/7866), done.\n",
            "Updating files: 100% (33/33), done.\n",
            "/content/medium-skill-based-agents\n",
            " agents\t\t\t\t    pyproject.toml     train_autoenc.py\n",
            " configs.yaml\t\t\t    README.md\t       train_usr.py\n",
            " convert_to_pt.py\t\t    skills\t       train_vok.py\n",
            " create_dataset.py\t\t    test_packages.py   train_vos.py\n",
            " environment_configs\t\t    test_train.py      utils\n",
            " MANIFEST.in\t\t\t    test_vok.ipynb     uv.lock\n",
            "'[Medium]Skilled_RL_Agents.ipynb'   test_vos.ipynb\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/Sopralapanca/medium-skill-based-agents.git\n",
        "# %cd /content/medium-skill-based-agents\n",
        "# !ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UREODkjw6ZL",
        "outputId": "956bf22f-297e-4f4f-c6ac-a695484b050e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "uv 0.9.17\n"
          ]
        }
      ],
      "source": [
        "!uv --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3VGFLkD12DN"
      },
      "outputs": [],
      "source": [
        "# ! yes | pip uninstall gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkFEB459xstf",
        "outputId": "9b872278-6cb9-4c2c-96b8-80d8d98e25bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m148 packages\u001b[0m \u001b[2min 7.55s\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m45 packages\u001b[0m \u001b[2min 1m 19s\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m11 packages\u001b[0m \u001b[2min 1.98s\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m45 packages\u001b[0m \u001b[2min 990ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1matariari\u001b[0m\u001b[2m==0.0.1 (from git+https://github.com/Sopralapanca/atari-representation-learning.git@9cc4c53fa44f45c14061343b57e8a40e25945028)\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcolorlog\u001b[0m\u001b[2m==6.10.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcomm\u001b[0m\u001b[2m==0.2.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdataproperty\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mgym\u001b[0m\u001b[2m==0.25.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgym\u001b[0m\u001b[2m==0.26.2\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mgymnasium\u001b[0m\u001b[2m==1.2.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgymnasium\u001b[0m\u001b[2m==1.2.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhuggingface-sb3\u001b[0m\u001b[2m==3.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mipykernel\u001b[0m\u001b[2m==6.17.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mipykernel\u001b[0m\u001b[2m==7.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjedi\u001b[0m\u001b[2m==0.19.2\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mjupyter-client\u001b[0m\u001b[2m==7.4.9\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjupyter-client\u001b[0m\u001b[2m==8.7.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mkornia\u001b[0m\u001b[2m==0.8.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mkornia-rs\u001b[0m\u001b[2m==0.1.10\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmbstrdecoder\u001b[0m\u001b[2m==1.1.4\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mmoviepy\u001b[0m\u001b[2m==1.0.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmoviepy\u001b[0m\u001b[2m==2.2.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.8.4.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.8.93\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.10.2.21\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.3.3.83\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufile-cu12\u001b[0m\u001b[2m==1.13.1.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.9.90\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.7.3.90\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.8.93\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.7.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.28.9\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.27.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.8.93\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvshmem-cu12\u001b[0m\u001b[2m==3.3.20\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1moptuna\u001b[0m\u001b[2m==4.6.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpathvalidate\u001b[0m\u001b[2m==3.3.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpytablewriter\u001b[0m\u001b[2m==1.2.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrl-zoo3\u001b[0m\u001b[2m==2.7.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msb3-contrib\u001b[0m\u001b[2m==2.7.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mscikit-learn\u001b[0m\u001b[2m==1.6.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mscikit-learn\u001b[0m\u001b[2m==1.8.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mshimmy\u001b[0m\u001b[2m==2.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mstable-baselines3\u001b[0m\u001b[2m==2.7.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtabledata\u001b[0m\u001b[2m==1.3.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtcolorpy\u001b[0m\u001b[2m==0.1.7\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtensorboard\u001b[0m\u001b[2m==2.19.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtensorboard\u001b[0m\u001b[2m==2.20.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtensorflow\u001b[0m\u001b[2m==2.19.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtensorflow\u001b[0m\u001b[2m==2.20.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.9.0+cpu\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.9.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.24.0+cpu\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.24.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.5.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtypepy\u001b[0m\u001b[2m==1.3.4\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# !uv pip install -r pyproject.toml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQ-Fqi6A2H5t"
      },
      "outputs": [],
      "source": [
        "# !python create_dataset.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PiyVRcgMUdK"
      },
      "outputs": [],
      "source": [
        "# !python train_vos.py\n",
        "# !python train_vok.py\n",
        "# !python train_usr.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_C88ROwMUdL"
      },
      "source": [
        "Import the required packages, build the environment and test if it works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1xZN1JaoBj4",
        "outputId": "19907a38-ba08-464a-9825-9b18e54c4154"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
          ]
        }
      ],
      "source": [
        "# general imports\n",
        "import torch\n",
        "import yaml\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "\n",
        "# training imports\n",
        "from stable_baselines3.common.env_util import make_atari_env\n",
        "from stable_baselines3.common.vec_env import VecFrameStack, VecTransposeImage\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# IMPORTANT - REGISTER THE ENVIRONMENTS\n",
        "import gymnasium as gym\n",
        "import ale_py\n",
        "gym.register_envs(ale_py)\n",
        "\n",
        "# Load config\n",
        "_config_path = \"./configs.yaml\"\n",
        "\n",
        "_config = {}\n",
        "with open(_config_path, \"r\") as f:\n",
        "    _config = yaml.safe_load(f) or {}\n",
        "\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  # ignore tensorflow warnings about CPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "seed = None\n",
        "if seed is not None:\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "\n",
        "#envs = _config.get(\"ENVS\", [\"PongNoFrameskip-v4\"])[0]\n",
        "env = \"PongNoFrameskip-v4\"\n",
        "with open(f'environment_configs/{env}.yaml', 'r') as file:\n",
        "        environment_configuration = yaml.safe_load(file)[\"config\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_env(env_id, configs, seed=None):\n",
        "    env = make_atari_env(env_id, n_envs=configs[\"n_envs\"], seed=seed)\n",
        "    env = VecFrameStack(env, n_stack=configs[\"n_stacks\"])\n",
        "    env = VecTransposeImage(env)\n",
        "    return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KjcYdvrRWjXI"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "key = os.getenv(\"WANDB_API_KEY\")\n",
        "if key is None:\n",
        "    raise ValueError(\"WANDB_API_KEY not set\")\n",
        "\n",
        "def init_wandb(environment_configuration):\n",
        "  wandb.login(key=key)\n",
        "\n",
        "  tags = [\n",
        "      f\"fe:{environment_configuration['f_ext_name']}\",\n",
        "      f\"game:{environment_configuration['game']}\",\n",
        "  ]\n",
        "\n",
        "  run = wandb.init(\n",
        "      project=\"medium-skill-based-agents\",\n",
        "      config=environment_configuration,\n",
        "      sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
        "      monitor_gym=False,  # auto-upload the videos of agents playing the game\n",
        "      group=f\"{environment_configuration['game']}\",\n",
        "      tags=tags\n",
        "      # save_code = True,  # optional\n",
        "  )\n",
        "\n",
        "  return run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "wolJs-iNMUdL",
        "outputId": "380143db-de7f-4eb8-d65b-23f0f6b49e2d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A.L.E: Arcade Learning Environment (version 0.11.2+ecc1138)\n",
            "[Powered by Stella]\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGgCAYAAADsNrNZAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAIkhJREFUeJzt3X901NWd//FXkkkmkWQmJMIMqQlEl25QZMUgYYS1LcbNoRwLJbq1h7ZYOGXVQIWc1ZpdQ9dVDNKuIK7A6nGjnkpZ8z2VFvcUj8YaD2v4FYuVKgFr1qTCDLU1MyGaCWTu95921jEgTJLJzYTn45x7Dp/7ufOZ91wGXufO5zOfSTHGGAEAMMxSbRcAALgwEUAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsSFkCPPfaYJk2apMzMTJWVlWnfvn2JeioAQBJKScS94P7rv/5L3/nOd7R161aVlZVp48aNamhoUGtrq8aPH/+5j41EIjp27JhycnKUkpIy1KUBABLMGKOuri4VFBQoNfVz1jkmAWbOnGmqqqqi2319faagoMDU1dWd87EdHR1GEo1Go9GSvHV0dHzu//eOoU6+3t5etbS0qKamJtqXmpqq8vJyNTc39xsfDocVDodjklOS5uircih9ULWkTJ8Ss903ZnDHSwZ9zrR+fR9OzRiW585751S/vvSPTw/Lc+PCcab3+B8vH573+NhW3uPn4/TpsF7fs145OTmfO27IA+jDDz9UX1+fPB5PTL/H49Hhw4f7ja+rq9N99913hsLS5UgZZAClOWO3HcPzJrUpxdH/H2eac3hetyO9/3M7HPzjxNDiPZ48znUaZcgDKF41NTWqrq6ObodCIRUWFioyZ5oijkyrtSWjSFr/v/DuiX3D8ty5v+v/WW9697A8NS4gZ3qPn5w0PO9xdxvv8aE05AF08cUXKy0tTYFAIKY/EAjI6/X2G+90OuV0Ovv1AwBGtyG/DDsjI0OlpaVqbGyM9kUiETU2Nsrn8w310wEAklRCPoKrrq7WkiVLNGPGDM2cOVMbN25Ud3e3vvvd7ybi6QAASSghAfSNb3xDf/jDH7RmzRr5/X5dddVV2rVrV78LEwAAF66EXYSwYsUKrVixIlGHxxC6rKH3nGPabux/ni6SaRJUETC0Lvt/536P/+/8/hc99WVFElQRxL3gAAC2EEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArEjYD9IhefTkZ5x7UMpwVAIkxvm8x00qP7A43FgBAQCsIIAAAFYQQAAAKwggAIAVXIQAfTD3fEZxghbJ64OvnM8o3uPDjRUQAMAKAggAYAUBBACwgnNAo0xaONKvL+/Xw/PX7Og+NSzPgwvbGd/jB3mPJyNWQAAAKwggAIAVBBAAwAoCCABgxYi9CKFtQYZSs87jLs04D/1P2ibCn65KO0PvmfqAocZ7fCSJfBKRdp97HCsgAIAVBBAAwIq4A+i1117TjTfeqIKCAqWkpGjHjh0x+40xWrNmjSZMmKCsrCyVl5fr6NGjQ1kzAGAUiPscUHd3t/7mb/5GS5cu1aJFi/rtX79+vTZt2qSnn35axcXFqq2tVUVFhd5++21lZmae9/P8ZkG9XDks0AAg2YS6Ihp717nHxR1A8+bN07x58864zxijjRs36t5779WCBQskSc8884w8Ho927NihW265Jd6nAwCMUkO6xGhra5Pf71d5eXm0z+12q6ysTM3NzWd8TDgcVigUimkAgNFvSAPI7/dLkjweT0y/x+OJ7vusuro6ud3uaCssLBzKkgAAI5T1kyw1NTUKBoPR1tHRYbskAMAwGNIA8nq9kqRAIBDTHwgEovs+y+l0yuVyxTQAwOg3pAFUXFwsr9erxsbGaF8oFNLevXvl8/mG8qkAAEku7qvgTp48qXfffTe63dbWpoMHDyovL09FRUVatWqVHnjgAU2ePDl6GXZBQYEWLlw41LUDAJJY3AF04MABfeUrX4luV1dXS5KWLFmip556Snfffbe6u7u1fPlydXZ2as6cOdq1a1dc3wECAIx+KcYYY7uITwuFQnK73froyKV8ERUAklCoK6KxX3xPwWDwc8/r8z88AMAKAggAYAUBBACwYsT+IN0Nh26UY4zTdhkAYMXEnI9itidnn4jZ/v0nY/s95nDn+ITXdT5Od4clPXLOcayAAABWEEAAACsIIACAFQQQAMCKEXsRwkUPu+RwcPcEABemd6bG3sB5T8kXY7az/zet32M8Bz5JeF3n4/TpnvMaxwoIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADAihH7g3QAcCHLPt4Xs53WE7tecIb6lOxYAQEArCCAAABWEEAAACsIIACAFVyEAAAjUOYfez+zba2UhGEFBACwggACAFgRVwDV1dXpmmuuUU5OjsaPH6+FCxeqtbU1ZkxPT4+qqqqUn5+v7OxsVVZWKhAIDHXdAIAkF1cANTU1qaqqSnv27NFLL72kU6dO6e/+7u/U3d0dHbN69Wrt3LlTDQ0Nampq0rFjx7Ro0aJE1A4ASGIpxhgz0Af/4Q9/0Pjx49XU1KTrrrtOwWBQ48aN07Zt23TTTTdJkg4fPqwpU6aoublZs2bNOucxQ6GQ3G63rptTK4cjc6ClAQAsOX26R6/tvl/BYFAul+us4wZ1DigYDEqS8vLyJEktLS06deqUysvLo2NKSkpUVFSk5ubmMx4jHA4rFArFNADA6DfgAIpEIlq1apVmz56tqVOnSpL8fr8yMjKUm5sbM9bj8cjv95/xOHV1dXK73dFWWFg40JIAAElkwAFUVVWlQ4cOafv27YMqoKamRsFgMNo6OjoGdTwAQHIY0BdRV6xYoRdeeEGvvfaaLrnkkmi/1+tVb2+vOjs7Y1ZBgUBAXq/3jMdyOp1yOp0DKQMAkMTiWgEZY7RixQo9//zzeuWVV1RcXByzv7S0VOnp6WpsbIz2tba2qr29XT6fb+iqBgAkvbhWQFVVVdq2bZt+/vOfKycnJ3pex+12KysrS263W8uWLVN1dbXy8vLkcrm0cuVK+Xy+87oCDgBw4YgrgLZs2SJJ+vKXvxzTX19fr1tvvVWStGHDBqWmpqqyslLhcFgVFRXavHnzUNYMABgF4gqg8/nKUGZmph577DE99thjg6kLADDKcS84AIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVcQXQli1bNG3aNLlcLrlcLvl8Pv3yl7+M7u/p6VFVVZXy8/OVnZ2tyspKBQKBRNQNAEhycQXQJZdconXr1qmlpUUHDhzQ3LlztWDBAv32t7+VJK1evVo7d+5UQ0ODmpqadOzYMS1atChRtQMAkliKMcYM5gB5eXn60Y9+pJtuuknjxo3Ttm3bdNNNN0mSDh8+rClTpqi5uVmzZs06r+OFQiG53W5dN6dWDkfmYEoDAFhw+nSPXtt9v4LBoFwu11nHDfgcUF9fn7Zv367u7m75fD61tLTo1KlTKi8vj44pKSlRUVGRmpubz3qccDisUCgU0wAAo1/cAfTWW28pOztbTqdTt912m55//nldfvnl8vv9ysjIUG5ubsx4j8cjv99/1uPV1dXJ7XZHW2Fh4cBeCQAgqcQdQH/913+tgwcPau/evbr99tu1ZMkSvf322wMuoKamRsFgMNo6OjoGfCwAQPJwxPuAjIwM/dVf/ZUkqbS0VPv379cjjzyib3zjG+rt7VVnZ2fMKigQCMjr9Z71eE6nU06nc6D1AwCS1KC/BxSJRBQOh1VaWqr09HQ1NjZG97W2tqq9vV0+n2+wTwMAGGXiWgHV1NRo3rx5KioqUldXl7Zt26ZXX31VL774otxut5YtW6bq6mrl5eXJ5XJp5cqV8vl8530FHADgwhFXAJ04cULf+c53dPz4cbndbk2bNk0vvviibrjhBknShg0blJqaqsrKSoXDYVVUVGjz5s2Jqh0AkMQG/T2gocb3gAAguSX8e0AAAAwGAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGDFoAJo3bp1SklJ0apVq6J9PT09qqqqUn5+vrKzs1VZWalAIDAUtQIARpEBB9D+/fv1H//xH5o2bVpM/+rVq7Vz5041NDSoqalJx44d06JFi4aiVgDAKDKgADp58qQWL16sJ554QmPHjo32B4NBPfnkk3r44Yc1d+5clZaWqr6+Xq+//rr27NkzlHUDAJLcgAKoqqpK8+fPV3l5eUx/S0uLTp06FdNfUlKioqIiNTc3n/FY4XBYoVAopgEARj9HvA/Yvn273njjDe3fv7/fPr/fr4yMDOXm5sb0ezwe+f3+Mx6vrq5O9913X7xlAACSXFwroI6ODt1555169tlnlZmZOSQF1NTUKBgMRltHR8eQHBcAMLLFFUAtLS06ceKErr76ajkcDjkcDjU1NWnTpk1yOBzyeDzq7e1VZ2dnzOMCgYC8Xu8Zj+l0OuVyuWIaAGD0i+sjuOuvv15vvfVWTN93v/tdlZSU6Ac/+IEKCwuVnp6uxsZGVVZWSpJaW1vV3t4un883tJUDAJJaXAGUk5OjqVOnxvSNGTNG+fn50f5ly5apurpaeXl5crlcWrlypXw+n2bNmjW0lQMAklrcFyGcy4YNG5SamqrKykqFw2FVVFRo8+bNQ/00AIAkl2KMMbaL+LRQKCS3263r5tTK4RiaCx0AAMPn9Okevbb7fgWDwc89r8+94AAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsMJhu4CR7sMrsvr1fVxgYrZz2mL3j323J9FlIUF+/5XYv++x70Ritsf4w8NcETB6sQICAFhBAAEArIgrgP7lX/5FKSkpMa2kpCS6v6enR1VVVcrPz1d2drYqKysVCAQSUTcAIMnFfQ7oiiuu0Msvv/x/B3D83yFWr16t//7v/1ZDQ4PcbrdWrFihRYsW6X/+53+GruJhdnpM/75TubHnBfqy0oavIAyZ33+5//m9d5Zvjtm+4tE7YrbH+BNeFnDBiDuAHA6HvF5vv/5gMKgnn3xS27Zt09y5cyVJ9fX1mjJlivbs2aNZs2YNTcUAgFEh7nNAR48eVUFBgS699FItXrxY7e3tkqSWlhadOnVK5eXl0bElJSUqKipSc3PzWY8XDocVCoViGgBg9IsrgMrKyvTUU09p165d2rJli9ra2vS3f/u36urqkt/vV0ZGhnJzc2Me4/F45Pef/XOLuro6ud3uaCssLBz4qwEAJI24PoKbN29e9M/Tpk1TWVmZJk6cqOeee05ZWf0/Tz8fNTU1qq6ujm6HQiFCCAAuAIP6Impubq6++MUv6t1339UNN9yg3t5edXZ2xqyCAoHAGc8Z/YXT6ZTT6RxMGcCAXOQ3/fomv3przPYXfnNqGCsCLiyD+h7QyZMn9bvf/U4TJkxQaWmp0tPT1djYGN3f2tqq9vZ2+Xy+oagVADCKxLUC+sd//EfdeOONmjhxoo4dO6Yf/vCHSktL0ze/+U253W4tW7ZM1dXVysvLk8vl0sqVK+Xz+bgCDgDQT1wB9Pvf/17f/OY39cc//lHjxo3TnDlztGfPHo0bN06StGHDBqWmpqqyslLhcFgVFRXavHnzOY8LALjwpBhj+n8QblEoFJLb7dZ1c2rlcGTaLkf+mf0vruie2Bez7T4c+0XUi9/6JOF1AcBIdfp0j17bfb+CwaBcLtdZx3EvOACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFXH9JPeFyPV+pF+f80+xuZ31UV+/MQCAz8cKCABgBQEEALCCAAIAWME5oHO4KBA+Q5+VUgBgVGEFBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArIg7gD744AN961vfUn5+vrKysnTllVfqwIED0f3GGK1Zs0YTJkxQVlaWysvLdfTo0aGuGwCQ5OIKoI8++kizZ89Wenq6fvnLX+rtt9/Wv/3bv2ns2LHRMevXr9emTZu0detW7d27V2PGjFFFRYV6enoSUT8AIEnFdTfshx56SIWFhaqvr4/2FRcXR/9sjNHGjRt17733asGCBZKkZ555Rh6PRzt27NAtt9wylLUDAJJYXCugX/ziF5oxY4ZuvvlmjR8/XtOnT9cTTzwR3d/W1ia/36/y8vJon9vtVllZmZqbm894zHA4rFAoFNMAAKNfXAH03nvvacuWLZo8ebJefPFF3X777fr+97+vp59+WpLk9/slSR6PJ+ZxHo8nuu+z6urq5Ha7o62wsHDgrwYAkDTiCqBIJKKrr75aDz74oKZPn67ly5fre9/7nrZu3TrgAmpqahQMBqOto6NjwMcCACSPuAJowoQJuvzyy2P6pkyZovb2dkmS1+uVJAUCsT8ZGggEovs+y+l0yuVyxTQAwOgXVwDNnj1bra2tMX1HjhzRxIkTpT9fkOD1etXY2BjdHwqFtHfvXvl8vqGqGQAwCsR1Fdzq1at17bXX6sEHH9Tf//3fa9++fXr88cf1+OOPS5JSUlK0atUqPfDAA5o8ebKKi4tVW1urgoICLVy4MFGvAQCQhOIKoGuuuUbPP/+8ampq9K//+q8qLi7Wxo0btXjx4uiYu+++W93d3Vq+fLk6Ozs1Z84c7dq1S5mZmYmoHwCQpFKMMcZ2EZ8WCoXkdrt13ZxaORyEFgAkm9One/Ta7vsVDAY/97w+94IDAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFgRVwBNmjRJKSkp/VpVVZUkqaenR1VVVcrPz1d2drYqKysVCAQSVTsAIInFFUD79+/X8ePHo+2ll16SJN18882SpNWrV2vnzp1qaGhQU1OTjh07pkWLFiWmcgBAUnPEM3jcuHEx2+vWrdNll12mL33pSwoGg3ryySe1bds2zZ07V5JUX1+vKVOmaM+ePZo1a9bQVg4ASGoDPgfU29urn/zkJ1q6dKlSUlLU0tKiU6dOqby8PDqmpKRERUVFam5uPutxwuGwQqFQTAMAjH4DDqAdO3aos7NTt956qyTJ7/crIyNDubm5MeM8Ho/8fv9Zj1NXVye32x1thYWFAy0JAJBEBhxATz75pObNm6eCgoJBFVBTU6NgMBhtHR0dgzoeACA5xHUO6C/ef/99vfzyy/rZz34W7fN6vert7VVnZ2fMKigQCMjr9Z71WE6nU06ncyBlAACS2IBWQPX19Ro/frzmz58f7SstLVV6eroaGxujfa2trWpvb5fP5xuaagEAo0bcK6BIJKL6+notWbJEDsf/PdztdmvZsmWqrq5WXl6eXC6XVq5cKZ/PxxVwAIB+4g6gl19+We3t7Vq6dGm/fRs2bFBqaqoqKysVDodVUVGhzZs3D1WtAIBRJMUYY2wX8WmhUEhut1vXzamVw5FpuxwAQJxOn+7Ra7vvVzAYlMvlOus47gUHALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKxy2CzibtgUZSs3KsF0GACBOkU8i0u5zj2MFBACwggACAFhBAAEArCCAAABWpBhjjO0iPi0UCsntduujI5fKlUM+AkCyCXVFNPaL7ykYDMrlcp11HP/DAwCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgRVwB1NfXp9raWhUXFysrK0uXXXaZ7r//fn36Sm5jjNasWaMJEyYoKytL5eXlOnr0aCJqBwAksbgC6KGHHtKWLVv07//+73rnnXf00EMPaf369Xr00UejY9avX69NmzZp69at2rt3r8aMGaOKigr19PQkon4AQJKK627Yr7/+uhYsWKD58+dLkiZNmqSf/vSn2rdvn/Tn1c/GjRt17733asGCBZKkZ555Rh6PRzt27NAtt9ySiNcAAEhCca2Arr32WjU2NurIkSOSpDfffFO7d+/WvHnzJEltbW3y+/0qLy+PPsbtdqusrEzNzc1nPGY4HFYoFIppAIDRL64V0D333KNQKKSSkhKlpaWpr69Pa9eu1eLFiyVJfr9fkuTxeGIe5/F4ovs+q66uTvfdd9/AXwEAICnFtQJ67rnn9Oyzz2rbtm1644039PTTT+vHP/6xnn766QEXUFNTo2AwGG0dHR0DPhYAIHnEtQK66667dM8990TP5Vx55ZV6//33VVdXpyVLlsjr9UqSAoGAJkyYEH1cIBDQVVdddcZjOp1OOZ3Owb0KAEDSiWsF9PHHHys1NfYhaWlpikQikqTi4mJ5vV41NjZG94dCIe3du1c+n2+oagYAjAJxrYBuvPFGrV27VkVFRbriiiv061//Wg8//LCWLl0qSUpJSdGqVav0wAMPaPLkySouLlZtba0KCgq0cOHCRL0GAEASiiuAHn30UdXW1uqOO+7QiRMnVFBQoH/4h3/QmjVromPuvvtudXd3a/ny5ers7NScOXO0a9cuZWZmJqJ+AECS4gfpAABDih+kAwCMaAQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWxPVF1OHwl68lhU5GbJcCABiAv/z/fa6vmY64AOrq6pIkTbz6f22XAgAYhK6uLrnd7rPuH3F3QohEIjp27JhycnLU1dWlwsJCdXR0fO63aTEwoVCI+U0g5jexmN/EGsz8GmPU1dWlgoKCfjew/rQRtwJKTU3VJZdcIv355qaS5HK5eIMlEPObWMxvYjG/iTXQ+f28lc9fcBECAMAKAggAYMWIDiCn06kf/vCH/GJqgjC/icX8Jhbzm1jDMb8j7iIEAMCFYUSvgAAAoxcBBACwggACAFhBAAEArCCAAABWjNgAeuyxxzRp0iRlZmaqrKxM+/bts11SUqqrq9M111yjnJwcjR8/XgsXLlRra2vMmJ6eHlVVVSk/P1/Z2dmqrKxUIBCwVnOyWrdunVJSUrRq1apoH3M7eB988IG+9a1vKT8/X1lZWbryyit14MCB6H5jjNasWaMJEyYoKytL5eXlOnr0qNWak0VfX59qa2tVXFysrKwsXXbZZbr//vtjbiKa0Pk1I9D27dtNRkaG+c///E/z29/+1nzve98zubm5JhAI2C4t6VRUVJj6+npz6NAhc/DgQfPVr37VFBUVmZMnT0bH3HbbbaawsNA0NjaaAwcOmFmzZplrr73Wat3JZt++fWbSpElm2rRp5s4774z2M7eD86c//clMnDjR3HrrrWbv3r3mvffeMy+++KJ59913o2PWrVtn3G632bFjh3nzzTfN1772NVNcXGw++eQTq7Ung7Vr15r8/HzzwgsvmLa2NtPQ0GCys7PNI488Eh2TyPkdkQE0c+ZMU1VVFd3u6+szBQUFpq6uzmpdo8GJEyeMJNPU1GSMMaazs9Okp6ebhoaG6Jh33nnHSDLNzc0WK00eXV1dZvLkyeall14yX/rSl6IBxNwO3g9+8AMzZ86cs+6PRCLG6/WaH/3oR9G+zs5O43Q6zU9/+tNhqjJ5zZ8/3yxdujSmb9GiRWbx4sXGDMP8jriP4Hp7e9XS0qLy8vJoX2pqqsrLy9Xc3Gy1ttEgGAxKkvLy8iRJLS0tOnXqVMx8l5SUqKioiPk+T1VVVZo/f37MHIq5HRK/+MUvNGPGDN18880aP368pk+frieeeCK6v62tTX6/P2aO3W63ysrKmOPzcO2116qxsVFHjhyRJL355pvavXu35s2bJw3D/I64u2F/+OGH6uvrk8fjien3eDw6fPiwtbpGg0gkolWrVmn27NmaOnWqJMnv9ysjI0O5ubkxYz0ej/x+v6VKk8f27dv1xhtvaP/+/f32MbeD995772nLli2qrq7WP/3TP2n//v36/ve/r4yMDC1ZsiQ6j2f6/4I5Prd77rlHoVBIJSUlSktLU19fn9auXavFixdLf34PK4HzO+ICCIlTVVWlQ4cOaffu3bZLGRU6Ojp055136qWXXlJmZqbtckalSCSiGTNm6MEHH5QkTZ8+XYcOHdLWrVu1ZMkS2+Ulveeee07PPvustm3bpiuuuEIHDx7UqlWrVFBQMCzzO+I+grv44ouVlpbW70qhQCAgr9drra5kt2LFCr3wwgv61a9+Ff29JUnyer3q7e1VZ2dnzHjm+9xaWlp04sQJXX311XI4HHI4HGpqatKmTZvkcDjk8XiY20GaMGGCLr/88pi+KVOmqL29Xfrz+1d/ntNPY47Pz1133aV77rlHt9xyi6688kp9+9vf1urVq1VXVycNw/yOuADKyMhQaWmpGhsbo32RSESNjY3y+XxWa0tGxhitWLFCzz//vF555RUVFxfH7C8tLVV6enrMfLe2tqq9vZ35Pofrr79eb731lg4ePBhtM2bM0OLFi6N/Zm4HZ/bs2f2+NnDkyBFNnDhRklRcXCyv1xszx6FQSHv37mWOz8PHH3/c7xdL09LSFIlEpOGY30FfxpAA27dvN06n0zz11FPm7bffNsuXLze5ubnG7/fbLi3p3H777cbtdptXX33VHD9+PNo+/vjj6JjbbrvNFBUVmVdeecUcOHDA+Hw+4/P5rNadrD59FZxhbgdt3759xuFwmLVr15qjR4+aZ5991lx00UXmJz/5SXTMunXrTG5urvn5z39ufvOb35gFCxZwGfZ5WrJkifnCF74QvQz7Zz/7mbn44ovN3XffHR2TyPkdkQFkjDGPPvqoKSoqMhkZGWbmzJlmz549tktKSpLO2Orr66NjPvnkE3PHHXeYsWPHmosuush8/etfN8ePH7dad7L6bAAxt4O3c+dOM3XqVON0Ok1JSYl5/PHHY/ZHIhFTW1trPB6PcTqd5vrrrzetra3W6k0moVDI3HnnnaaoqMhkZmaaSy+91PzzP/+zCYfD0TGJnF9+DwgAYMWIOwcEALgwEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFf8f2bycnQ75mHoAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "test_envs = create_env(env_id=env, configs=environment_configuration, seed=seed)\n",
        "\n",
        "# execute some steps with random moves\n",
        "obs = test_envs.reset()\n",
        "\n",
        "for i in range(10):\n",
        "    action = [test_envs.action_space.sample() for _ in range(environment_configuration[\"n_envs\"])]\n",
        "    obs, rewards, dones, info = test_envs.step(action)\n",
        "\n",
        "# obs[0] has shape (4, 84, 84) because there are 4 stacked environments, take the first\n",
        "observation = obs[0][-1]\n",
        "plt.imshow(observation)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Utility function for training agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/giacomo/Documents/code-projects/medium-skill-based-agents/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold, StopTrainingOnNoModelImprovement\n",
        "from wandb.integration.sb3 import WandbCallback\n",
        "from stable_baselines3 import PPO\n",
        "from rl_zoo3.utils import linear_schedule\n",
        "\n",
        "\n",
        "def train_agent(env_id, configs, policy_kwargs, seed):\n",
        "    \n",
        "    #run = init_wandb(configs)\n",
        "    logdir = \"./tensorboard_logs\"\n",
        "    \n",
        "    # monitor_dir = str(run.id)\n",
        "    monitor_dir = \"ppo\"\n",
        "\n",
        "    vec_envs = create_env(env_id=env_id, configs=configs, seed=seed)\n",
        "    _ = vec_envs.reset()\n",
        "    \n",
        "    eval_envs = create_env(env_id=env_id, configs=configs, seed=None)\n",
        "\n",
        "    model = PPO(\n",
        "        \"CnnPolicy\",\n",
        "        vec_envs,\n",
        "        learning_rate=linear_schedule(environment_configuration[\"learning_rate\"]),\n",
        "        n_steps=environment_configuration[\"n_steps\"],\n",
        "        n_epochs=environment_configuration[\"n_epochs\"],\n",
        "        batch_size=environment_configuration[\"batch_size\"],\n",
        "        clip_range=linear_schedule(environment_configuration[\"clip_range\"]),\n",
        "        normalize_advantage=environment_configuration[\"normalize\"],\n",
        "        ent_coef=environment_configuration[\"ent_coef\"],\n",
        "        vf_coef=environment_configuration[\"vf_coef\"],\n",
        "        policy_kwargs=policy_kwargs,\n",
        "        verbose=1,\n",
        "        device=device,\n",
        "        tensorboard_log=logdir,\n",
        "    )\n",
        "\n",
        "\n",
        "    eval_logs = f\"eval_logs/{env}/{monitor_dir}\"\n",
        "    os.makedirs(eval_logs, exist_ok=True)\n",
        "\n",
        "    eval_callback = EvalCallback(\n",
        "        eval_envs,\n",
        "        n_eval_episodes=100,\n",
        "        best_model_save_path=f\"./agents/{monitor_dir}\",\n",
        "        log_path=eval_logs,\n",
        "        eval_freq=5000 * environment_configuration[\"n_envs\"],\n",
        "        verbose=0,\n",
        "    )\n",
        "\n",
        "    callbacks = [\n",
        "        #WandbCallback(verbose=0),\n",
        "        eval_callback\n",
        "    ]\n",
        "\n",
        "    model.learn(5000, callback=callbacks, progress_bar=True) #tb_log_name=run.id)\n",
        "    #run.finish()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train Standard PPO Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "policy_kwargs = dict(\n",
        "    net_arch={\n",
        "        \"pi\": environment_configuration[\"net_arch_pi\"],\n",
        "        \"vf\": environment_configuration[\"net_arch_vf\"],\n",
        "    },\n",
        "    # activation_fn=torch.nn.ReLU,  # use ReLU in case of multiple layers for the policy learning network\n",
        ")\n",
        "\n",
        "#train_agent(env, environment_configuration, policy_kwargs, seed)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_98s5PWMUdM",
        "outputId": "45789a98-3844-47b1-82fa-e3664cde07d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4, 84, 84)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/giacomo/Documents/code-projects/medium-skill-based-agents/.venv/lib/python3.12/site-packages/torch/functional.py:505: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4317.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing skill input adapters:\n",
            "\n",
            "Skill: state_rep_uns\n",
            "Input shape: torch.Size([1, 4, 84, 84])\n",
            "After adapter: torch.Size([1, 1, 160, 210])\n",
            "\n",
            "Skill: obj_key_enc\n",
            "Input shape: torch.Size([1, 4, 84, 84])\n",
            "After adapter: torch.Size([1, 1, 84, 84])\n",
            "\n",
            "Skill: obj_key_key\n",
            "Input shape: torch.Size([1, 4, 84, 84])\n",
            "After adapter: torch.Size([1, 1, 84, 84])\n",
            "\n",
            "Skill: vid_obj_seg\n",
            "Input shape: torch.Size([1, 4, 84, 84])\n",
            "After adapter: torch.Size([1, 2, 84, 84])\n"
          ]
        }
      ],
      "source": [
        "from skills.autoencoder import Autoencoder\n",
        "from skills.unsupervised_state_representation import UnsupervisedStateRepresentationModel\n",
        "from skills.video_object_keypoints import Transporter\n",
        "from skills.video_object_segmentation import VideoObjectSegmentationModel\n",
        "\n",
        "# init skills\n",
        "autoencoder = Autoencoder(channels=1).to(device)\n",
        "\n",
        "# observation has shape (32, 4, 84, 84),\n",
        "# observation = obs[0][0][None, :, :] # (1, 84, 84)\n",
        "\n",
        "print(obs[0].shape) # (4, 84, 84)\n",
        "\n",
        "environment_configuration[\"f_ext_kwargs\"][\"device\"] = device  #do not comment this, it is the parameter passed to the feature extractor\n",
        "environment_configuration[\"game\"] = env\n",
        "\n",
        "\n",
        "usr = UnsupervisedStateRepresentationModel(observation=obs[0], device=device)\n",
        "vok = Transporter().to(device)\n",
        "vos = VideoObjectSegmentationModel(device=device)\n",
        "\n",
        "\n",
        "skills = [\n",
        "    usr.get_skill(device=device),\n",
        "    vok.get_skill(device=device, keynet_or_encoder=\"encoder\"),\n",
        "    vok.get_skill(device=device, keynet_or_encoder=\"keynet\"),\n",
        "    vos.get_skill(device=device)\n",
        "]\n",
        "\n",
        "# Test each skill's input adapter\n",
        "print(\"\\nTesting skill input adapters:\")\n",
        "test_obs = obs[:1]  # Take one sample from batch\n",
        "test_obs = torch.tensor(test_obs, dtype=torch.float32).to(device)\n",
        "\n",
        "for skill in skills:\n",
        "    print(f\"\\nSkill: {skill.name}\")\n",
        "    print(f\"Input shape: {test_obs.shape}\")\n",
        "    adapted = skill.input_adapter(test_obs)\n",
        "    print(f\"After adapter: {adapted.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train WSA agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "Logging to ./tensorboard_logs/PPO_10\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 66   |\n",
            "|    iterations      | 1    |\n",
            "|    time_elapsed    | 15   |\n",
            "|    total_timesteps | 1024 |\n",
            "-----------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 19           |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 103          |\n",
            "|    total_timesteps      | 2048         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0008237411 |\n",
            "|    clip_fraction        | 0.123        |\n",
            "|    clip_range           | 0.0488       |\n",
            "|    entropy_loss         | -1.79        |\n",
            "|    explained_variance   | -0.013       |\n",
            "|    learning_rate        | 0.000122     |\n",
            "|    loss                 | 0.0236       |\n",
            "|    n_updates            | 4            |\n",
            "|    policy_gradient_loss | -0.000502    |\n",
            "|    value_loss           | 0.187        |\n",
            "------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from utils.feature_extractors import WeightSharingAttentionExtractor\n",
        "\n",
        "f_ext_kwargs = environment_configuration[\"f_ext_kwargs\"]\n",
        "environment_configuration[\"f_ext_name\"] = \"wsharing_attention_ext\"\n",
        "environment_configuration[\"f_ext_class\"] = WeightSharingAttentionExtractor\n",
        "f_ext_kwargs[\"skills\"] = skills\n",
        "f_ext_kwargs[\"features_dim\"] = 256\n",
        "\n",
        "policy_kwargs[\"features_extractor_class\"] = environment_configuration[\"f_ext_class\"]\n",
        "policy_kwargs[\"features_extractor_kwargs\"] = f_ext_kwargs\n",
        "\n",
        "train_agent(env, environment_configuration, policy_kwargs, seed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train MOE agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "Logging to ./tensorboard_logs/PPO_18\n",
            "Observations device: cpu\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'exit' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m policy_kwargs[\u001b[33m\"\u001b[39m\u001b[33mfeatures_extractor_class\u001b[39m\u001b[33m\"\u001b[39m] = environment_configuration[\u001b[33m\"\u001b[39m\u001b[33mf_ext_class\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     10\u001b[39m policy_kwargs[\u001b[33m\"\u001b[39m\u001b[33mfeatures_extractor_kwargs\u001b[39m\u001b[33m\"\u001b[39m] = f_ext_kwargs\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvironment_configuration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mtrain_agent\u001b[39m\u001b[34m(env_id, configs, policy_kwargs, seed)\u001b[39m\n\u001b[32m     41\u001b[39m eval_callback = EvalCallback(\n\u001b[32m     42\u001b[39m     eval_envs,\n\u001b[32m     43\u001b[39m     n_eval_episodes=\u001b[32m100\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     47\u001b[39m     verbose=\u001b[32m0\u001b[39m,\n\u001b[32m     48\u001b[39m )\n\u001b[32m     50\u001b[39m callbacks = [\n\u001b[32m     51\u001b[39m     \u001b[38;5;66;03m#WandbCallback(verbose=0),\u001b[39;00m\n\u001b[32m     52\u001b[39m     eval_callback\n\u001b[32m     53\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code-projects/medium-skill-based-agents/.venv/lib/python3.12/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[39m, in \u001b[36mPPO.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    303\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[32m    304\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    309\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    310\u001b[39m ) -> SelfPPO:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code-projects/medium-skill-based-agents/.venv/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:324\u001b[39m, in \u001b[36mOnPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_timesteps < total_timesteps:\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m     continue_training = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code-projects/medium-skill-based-agents/.venv/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:202\u001b[39m, in \u001b[36mOnPolicyAlgorithm.collect_rollouts\u001b[39m\u001b[34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m th.no_grad():\n\u001b[32m    200\u001b[39m     \u001b[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[32m    201\u001b[39m     obs_tensor = obs_as_tensor(\u001b[38;5;28mself\u001b[39m._last_obs, \u001b[38;5;28mself\u001b[39m.device)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     actions, values, log_probs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m actions = actions.cpu().numpy()\n\u001b[32m    205\u001b[39m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code-projects/medium-skill-based-agents/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code-projects/medium-skill-based-agents/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code-projects/medium-skill-based-agents/.venv/lib/python3.12/site-packages/stable_baselines3/common/policies.py:645\u001b[39m, in \u001b[36mActorCriticPolicy.forward\u001b[39m\u001b[34m(self, obs, deterministic)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    638\u001b[39m \u001b[33;03mForward pass in all the networks (actor and critic)\u001b[39;00m\n\u001b[32m    639\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    642\u001b[39m \u001b[33;03m:return: action, value and log probability of the action\u001b[39;00m\n\u001b[32m    643\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    644\u001b[39m \u001b[38;5;66;03m# Preprocess the observation if needed\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m645\u001b[39m features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.share_features_extractor:\n\u001b[32m    647\u001b[39m     latent_pi, latent_vf = \u001b[38;5;28mself\u001b[39m.mlp_extractor(features)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code-projects/medium-skill-based-agents/.venv/lib/python3.12/site-packages/stable_baselines3/common/policies.py:672\u001b[39m, in \u001b[36mActorCriticPolicy.extract_features\u001b[39m\u001b[34m(self, obs, features_extractor)\u001b[39m\n\u001b[32m    663\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    664\u001b[39m \u001b[33;03mPreprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[32m    665\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    669\u001b[39m \u001b[33;03m    features for the actor and the features for the critic.\u001b[39;00m\n\u001b[32m    670\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    671\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.share_features_extractor:\n\u001b[32m--> \u001b[39m\u001b[32m672\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures_extractor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures_extractor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures_extractor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    674\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m features_extractor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code-projects/medium-skill-based-agents/.venv/lib/python3.12/site-packages/stable_baselines3/common/policies.py:131\u001b[39m, in \u001b[36mBaseModel.extract_features\u001b[39m\u001b[34m(self, obs, features_extractor)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[33;03mPreprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[32m    125\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    128\u001b[39m \u001b[33;03m:return: The extracted features\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    130\u001b[39m preprocessed_obs = preprocess_obs(obs, \u001b[38;5;28mself\u001b[39m.observation_space, normalize_images=\u001b[38;5;28mself\u001b[39m.normalize_images)\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfeatures_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed_obs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code-projects/medium-skill-based-agents/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code-projects/medium-skill-based-agents/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code-projects/medium-skill-based-agents/utils/feature_extractors.py:305\u001b[39m, in \u001b[36mMixtureOfExpertsExtractor.forward\u001b[39m\u001b[34m(self, observations)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, observations: torch.Tensor) -> torch.Tensor:\n\u001b[32m    303\u001b[39m     \u001b[38;5;66;03m# print device of observations\u001b[39;00m\n\u001b[32m    304\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mObservations device:\u001b[39m\u001b[33m\"\u001b[39m, observations.device)\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m     \u001b[43mexit\u001b[49m()  \n\u001b[32m    308\u001b[39m     batch_size = observations.shape[\u001b[32m0\u001b[39m]\n\u001b[32m    310\u001b[39m     context = get_embedding_for_context(observations, \u001b[38;5;28mself\u001b[39m.encoder)\n",
            "\u001b[31mNameError\u001b[39m: name 'exit' is not defined"
          ]
        }
      ],
      "source": [
        "from utils.feature_extractors import MixtureOfExpertsExtractor\n",
        "\n",
        "f_ext_kwargs = environment_configuration[\"f_ext_kwargs\"]\n",
        "environment_configuration[\"f_ext_name\"] = \"moe_ext\"\n",
        "environment_configuration[\"f_ext_class\"] = MixtureOfExpertsExtractor\n",
        "f_ext_kwargs[\"skills\"] = skills\n",
        "f_ext_kwargs[\"features_dim\"] = 256\n",
        "\n",
        "policy_kwargs[\"features_extractor_class\"] = environment_configuration[\"f_ext_class\"]\n",
        "policy_kwargs[\"features_extractor_kwargs\"] = f_ext_kwargs\n",
        "\n",
        "train_agent(env, environment_configuration, policy_kwargs, seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "medium-skill-based-agents",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
