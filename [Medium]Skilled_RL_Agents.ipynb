{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0OpfDo6BptY"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIS7A-97w03Y",
        "outputId": "75f6dd4b-e180-4731-a030-7ffdca3eb5f9"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/Sopralapanca/medium-skill-based-agents.git\n",
        "# %cd /content/medium-skill-based-agents\n",
        "# !ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UREODkjw6ZL",
        "outputId": "8b81adc0-24b5-468e-dec2-7c9ea6498ec8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "uv 0.9.18 (0cee76417 2025-12-16)\n"
          ]
        }
      ],
      "source": [
        "!uv --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "T3VGFLkD12DN"
      },
      "outputs": [],
      "source": [
        "# ! yes | pip uninstall gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkFEB459xstf",
        "outputId": "edd35456-4996-4e49-d40b-15ded54316df"
      },
      "outputs": [],
      "source": [
        "# !uv pip install -r pyproject.toml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQ-Fqi6A2H5t",
        "outputId": "6b330c80-6639-4dab-f134-9d64cb8ae19b"
      },
      "outputs": [],
      "source": [
        "# !python create_dataset.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !python train_vos.py\n",
        "# !python train_vok.py\n",
        "# !python train_usr.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import the required packages, build the environment and test if it works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\giaco\\VSCode Projects\\medium-articles\\skill-based-agents\\.venv\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(np, \"object\"):\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGgCAYAAADsNrNZAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAH6tJREFUeJzt3QtQlNfdx/E/sNwiAoIK0oC3moK3JmKiGJo0SsNYx2oxNnZMg9GJTYJGYRoT2mCamgRj2mhMvTSORZ1orLwTTcxMzFhscJzgjdQ0xohaaaBBMGkDKIaL8Lxzzjvs6wpegF3OLvv9zJzZfS67eziu+9vznPM862NZliUAAHQz3+5+QQAAFAIIAGAEAQQAMIIAAgAYQQABAIwggAAARhBAAAAjCCAAgBEEEADACAIIANCzAmjNmjUyaNAgCQoKknHjxsnhw4dd9VIAAA/k44prwf3lL3+Rhx9+WNavX6/DZ9WqVZKfny8lJSXSv3//6z62paVFKioqpHfv3uLj4+PsqgEAXEzFyoULFyQmJkZ8fa/Tz7Fc4K677rIyMjLsy83NzVZMTIyVm5t7w8eWl5erQKRQKBSKeHZRn+fXY3N28jU2NkpxcbFkZ2fb16kETElJkaKiojb7NzQ06HJlcirJ8mOxiX+X6uJzR4LDcnOvrj2fJ2gO9Guz7uuRAd3y2hGfN7VZ53/pcre8NrxHe+/x/wzvnvd4nxLe4zfj8uUG+ejgCn0k63qcHkBff/21NDc3S1RUlMN6tXzy5Mk2++fm5srzzz/fTsX8xebTxQDyC3RctnXPm9QkH1vb/5x+gd3zd9v82762zcZ/TjgX73HPcaNhFKcHUEepnlJWVpZ9uba2VmJjY6UlebS02IKM1s0Ttfi1/QevG9jcLa8d/s+2x3r967rlpeHl7/GLg7rnPR5WynvcmZweQH379hU/Pz+pqqpyWK+Wo6Oj2+wfGBioCwDAuzh9GnZAQIAkJiZKQUGBw8w2tZyUlOTslwMAeCiXHIJTh9TS09Nl7Nixctddd+lp2HV1dfLII4+44uUAAB7IJQH04IMPyldffSVLly6VyspKuf3222XPnj1tJiYAALyXyyYhLFiwQBe4v6H5jTfcp3Rq23G6lqD/mzIPuLuh/3Pj9/i/prSd9NQc3OKiGkHhWnAAACMIIACAEQQQAMAIAggAYAQBBAAwggACABhBAAEAjCCAAABGEEAAACMIIACAEQQQAMAIAggAYAQBBAAwggACABhBAAEAjCCAAAA96wfp4DnqIwNuvJNPd9QEMPcet3z5gcXuRg8IAGAEAQQAMIIAAgAYQQABAIxgEgLky4k3sxcDtPBcX953M3vxHu9u9IAAAEYQQAAAIwggAIARjAH1MH4NLW3WRfy9e/6ZbXVN3fI68G7tvseP8R73RPSAAABGEEAAACMIIACAEQQQAMAIt52EUDotQHyDb+IqzbgJbQdtXeG/t/u1s7a9dYCz8R53Jy3ftogcuPF+9IAAAEYQQAAAzwig/fv3y9SpUyUmJkZ8fHxk165dDtsty5KlS5fKgAEDJDg4WFJSUuT06dPOrDMAwBvHgOrq6uT73/++zJ07V9LS0tpsX7FihaxevVo2b94sgwcPlpycHElNTZUTJ05IUFDQTb/OP6blSWhvOmgA4GlqL7RIn6dcEECTJ0/WpT2q97Nq1Sp59tlnZdq0aXrdli1bJCoqSveUZs2a1dGXAwD0UE7tYpSWlkplZaU+7NYqLCxMxo0bJ0VFRe0+pqGhQWprax0KAKDnc2oAqfBRVI/nSmq5ddvVcnNzdUi1ltjYWGdWCQDgpowPsmRnZ0tNTY29lJeXm64SAMDTAig6OlrfVlVVOaxXy63brhYYGCihoaEOBQDQ8zk1gNSsNxU0BQUF9nVqTOfQoUOSlJTkzJcCAHi4Ds+Cu3jxopw5c8Zh4sGxY8ckIiJC4uLiZPHixfLCCy/IsGHD7NOw1TlD06dPd3bdAQDeFEBHjx6V++67z76clZWlb9PT02XTpk2yZMkSfa7Q/Pnzpbq6WpKTk2XPnj0dOgcIANDz+Vjq5B03og7Zqdlw35wawomoAOCpJ6LedlZPLLveuD6f8AAAIwggAIARBBAAwAi3/UG6Hx2fKrZegaarAQDooMt1DSLy2g33owcEADCCAAIAGEEAAQCMIIAAAEa47SSEW14NFZuNqycAgKe5fLn+pvajBwQAMIIAAgAYQQABAIwggAAARhBAAAAjCCAAgBEEEADACAIIAGAEAQQAMIIAAgAYQQABAIwggAAARhBAAAAjCCAAgBEEEADACAIIAGAEAQQAMIIAAgAYQQABAIwggAAARhBAAAAjCCAAgBEEEADACAIIAOD+AZSbmyt33nmn9O7dW/r37y/Tp0+XkpISh33q6+slIyNDIiMjJSQkRGbMmCFVVVXOrjcAwJsCqLCwUIfLwYMHZe/evdLU1CT333+/1NXV2ffJzMyU3bt3S35+vt6/oqJC0tLSXFF3AIAH87Esy+rsg7/66ivdE1JBc88990hNTY3069dPtm3bJg888IDe5+TJk5KQkCBFRUUyfvz4Gz5nbW2thIWFyT3JOWKzBXW2agAAQy5frpf9B5bpTAgNDXXNGJB6ciUiIkLfFhcX615RSkqKfZ/4+HiJi4vTAdSehoYGHTpXFgBAz9fpAGppaZHFixfL3XffLSNHjtTrKisrJSAgQMLDwx32jYqK0tuuNa6kejytJTY2trNVAgB4QwCpsaDjx4/L9u3bu1SB7Oxs3ZNqLeXl5V16PgCAZ7B15kELFiyQ9957T/bv3y+33nqrfX10dLQ0NjZKdXW1Qy9IzYJT29oTGBioCwDAu3SoB6TmK6jw2blzp+zbt08GDx7ssD0xMVH8/f2loKDAvk5N0y4rK5OkpCTn1RoA4F09IHXYTc1we+edd/S5QK3jOmrsJjg4WN/OmzdPsrKy9MQENfth4cKFOnxuZgYcAMB7dCiA1q1bp29/+MMfOqzPy8uTOXPm6PsrV64UX19ffQKqmuGWmpoqa9eudWadAQDeFkA3c8pQUFCQrFmzRhcAAK6Fa8EBAIwggAAARhBAAAAjCCAAgBEEEADACAIIAGAEAQQAMIIAAgAYQQABAIwggAAARhBAAAAjCCAAgBEEEADACAIIAGAEAQQAMIIAAgAYQQABAIwggAAARhBAAAAjCCAAgBEEEADACAIIAGAEAQQAMIIAAgAYQQABAIwggAAARhBAAAAjCCAAgBEEEADACAIIAGAEAQQAMIIAAgAYQQABANw/gNatWyejR4+W0NBQXZKSkuT999+3b6+vr5eMjAyJjIyUkJAQmTFjhlRVVbmi3gAAbwqgW2+9VZYvXy7FxcVy9OhRmThxokybNk0+++wzvT0zM1N2794t+fn5UlhYKBUVFZKWluaqugMAPJiPZVlWV54gIiJCXnnlFXnggQekX79+sm3bNn1fOXnypCQkJEhRUZGMHz/+pp6vtrZWwsLC5J7kHLHZgrpSNQCAAZcv18v+A8ukpqZGHy1z+hhQc3OzbN++Xerq6vShONUrampqkpSUFPs+8fHxEhcXpwPoWhoaGnToXFkAAD1fhwPo008/1eM7gYGB8thjj8nOnTtl+PDhUllZKQEBARIeHu6wf1RUlN52Lbm5ubrH01piY2M795cAAHp2AH3ve9+TY8eOyaFDh+Txxx+X9PR0OXHiRKcrkJ2drbtpraW8vLzTzwUA8By2jj5A9XK++93v6vuJiYly5MgRee211+TBBx+UxsZGqa6udugFqVlw0dHR13w+1ZNSBQDgXbp8HlBLS4sex1Fh5O/vLwUFBfZtJSUlUlZWpseIAADodA9IHS6bPHmynlhw4cIFPePtww8/lA8++ECP38ybN0+ysrL0zDg182HhwoU6fG52BhwAwHt0KIDOnz8vDz/8sJw7d04HjjopVYXPj370I7195cqV4uvrq09AVb2i1NRUWbt2ravqDgDw5vOAnI3zgADAs7n8PCAAALqCAAIAGEEAAQCMIIAAAEYQQAAAIwggAIARBBAAwAgCCABgBAEEADCCAAIAGEEAAQCMIIAAAEYQQAAAIwggAIARBBAAwAgCCABgBAEEADCCAAIAGEEAAQCMIIAAAEYQQAAAIwggAIARBBAAwAgCCABgBAEEADCCAAIAGEEAAQCMIIAAAEYQQAAAIwggAIARBBAAwAgCCABgBAEEAPC8AFq+fLn4+PjI4sWL7evq6+slIyNDIiMjJSQkRGbMmCFVVVXOqCsAoAfpdAAdOXJE/vSnP8no0aMd1mdmZsru3bslPz9fCgsLpaKiQtLS0pxRVwCAtwfQxYsXZfbs2bJhwwbp06ePfX1NTY1s3LhRXn31VZk4caIkJiZKXl6efPTRR3Lw4EFn1hsA4I0BpA6xTZkyRVJSUhzWFxcXS1NTk8P6+Ph4iYuLk6Kionafq6GhQWprax0KAKDns3X0Adu3b5ePP/5YH4K7WmVlpQQEBEh4eLjD+qioKL2tPbm5ufL88893tBoAAG/qAZWXl8uiRYtk69atEhQU5JQKZGdn60N3rUW9BgCg5+tQAKlDbOfPn5cxY8aIzWbTRU00WL16tb6vejqNjY1SXV3t8Dg1Cy46Orrd5wwMDJTQ0FCHAgDo+Tp0CG7SpEny6aefOqx75JFH9DjP008/LbGxseLv7y8FBQV6+rVSUlIiZWVlkpSU5NyaAwC8J4B69+4tI0eOdFjXq1cvfc5P6/p58+ZJVlaWRERE6N7MwoULdfiMHz/euTUHAHjXJIQbWblypfj6+uoekJrhlpqaKmvXrnX2ywAAPJyPZVmWuBE1DTssLEzuSc4Rm805Ex0AAN3n8uV62X9gmZ5Ydr1xfa4FBwAwggACABhBAAEAjCCAAABGEEAAACMIIACAEQQQAMAIAggAYAQBBAAwggACABhBAAEAjCCAAABGEEAAACMIIACAEQQQAMAIAggAYAQBBAAwggACABhBAAEAjCCAAABGEEAAACMIIACAEQQQAMAIAggAYAQBBAAwggACABhBAAEAjCCAAABGEEAAACMIIACAEQQQAMAIAggAYAQBBABw/wD67W9/Kz4+Pg4lPj7evr2+vl4yMjIkMjJSQkJCZMaMGVJVVeWKegMAvK0HNGLECDl37py9HDhwwL4tMzNTdu/eLfn5+VJYWCgVFRWSlpbm7DoDAHoAW4cfYLNJdHR0m/U1NTWyceNG2bZtm0ycOFGvy8vLk4SEBDl48KCMHz/eOTUGAHhnD+j06dMSExMjQ4YMkdmzZ0tZWZleX1xcLE1NTZKSkmLfVx2ei4uLk6Kioms+X0NDg9TW1joUAEDP16EAGjdunGzatEn27Nkj69atk9LSUvnBD34gFy5ckMrKSgkICJDw8HCHx0RFRelt15KbmythYWH2Ehsb2/m/BgDQMw/BTZ482X5/9OjROpAGDhwoO3bskODg4E5VIDs7W7KysuzLqgdECAFAz9eladiqt3PbbbfJmTNn9LhQY2OjVFdXO+yjZsG1N2bUKjAwUEJDQx0KAKDn61IAXbx4Uf75z3/KgAEDJDExUfz9/aWgoMC+vaSkRI8RJSUlOaOuAABvPQT3q1/9SqZOnaoPu6kp1s8995z4+fnJz3/+cz1+M2/ePH04LSIiQvdkFi5cqMOHGXAAgC4F0L///W8dNv/5z3+kX79+kpycrKdYq/vKypUrxdfXV5+Aqma3paamytq1azvyEgAAL+FjWZYlbkRNQlC9qXuSc8RmCzJdHQBAB12+XC/7DyzT54deb1yfa8EBAIwggAAARhBAAAAjCCAAgBEEEADACAIIAGAEAQQAMIIAAgAYQQABAIwggAAARhBAAAAjCCAAgBEEEADACAIIAGAEAQQAMIIAAgAYQQABAIwggAAARhBAAAAjCCAAgBEEEADACAIIAGAEAQQAMIIAAgAYQQABAIwggAAARhBAAAAjCCAAgBEEEADACAIIAGAEAQQAMIIAAgAYQQABADwjgL788kt56KGHJDIyUoKDg2XUqFFy9OhR+3bLsmTp0qUyYMAAvT0lJUVOnz7t7HoDALwpgL755hu5++67xd/fX95//305ceKE/OEPf5A+ffrY91mxYoWsXr1a1q9fL4cOHZJevXpJamqq1NfXu6L+AAAPZevIzi+//LLExsZKXl6efd3gwYMdej+rVq2SZ599VqZNm6bXbdmyRaKiomTXrl0ya9YsZ9YdAOAtPaB3331Xxo4dKzNnzpT+/fvLHXfcIRs2bLBvLy0tlcrKSn3YrVVYWJiMGzdOioqK2n3OhoYGqa2tdSgAgJ6vQwF09uxZWbdunQwbNkw++OADefzxx+XJJ5+UzZs36+0qfBTV47mSWm7ddrXc3FwdUq1F9bAAAD1fhwKopaVFxowZIy+99JLu/cyfP18effRRPd7TWdnZ2VJTU2Mv5eXlnX4uAEAPDSA1s2348OEO6xISEqSsrEzfj46O1rdVVVUO+6jl1m1XCwwMlNDQUIcCAOj5OhRAagZcSUmJw7pTp07JwIED7RMSVNAUFBTYt6sxHTUbLikpyVl1BgB42yy4zMxMmTBhgj4E97Of/UwOHz4sb7zxhi6Kj4+PLF68WF544QU9TqQCKScnR2JiYmT69Omu+hsAAD09gO68807ZuXOnHrf53e9+pwNGTbuePXu2fZ8lS5ZIXV2dHh+qrq6W5ORk2bNnjwQFBbmi/gAAD+VjqZN33Ig6ZKdmw92TnCM2G6EFAJ7m8uV62X9gmZ5Ydr1xfa4FBwAwggACABhBAAEAjCCAAABGEEAAACMIIACAEQQQAMAIAggAYAQBBAAwggACABhBAAEAjCCAAABGEEAAACMIIACAEQQQAMAIAggAYAQBBAAwggACABhBAAEAjCCAAABGEEAAACMIIACAEQQQAMAIAggAYAQBBAAwggACABhhM/OyAIDrabrF8eO5Odixv+BX39LmMf51l8WT0AMCABhBAAEAjCCAAABGMAYEAG6oZoi/43J8s8NyyL8ctytRRxkDAgDghgggAID7B9CgQYPEx8enTcnIyNDb6+vr9f3IyEgJCQmRGTNmSFVVlavqDgDwlgA6cuSInDt3zl727t2r18+cOVPfZmZmyu7duyU/P18KCwuloqJC0tLSXFNzAID3TELo16+fw/Ly5ctl6NChcu+990pNTY1s3LhRtm3bJhMnTtTb8/LyJCEhQQ4ePCjjx493bs0BAN45BtTY2ChvvvmmzJ07Vx+GKy4ulqamJklJSbHvEx8fL3FxcVJUVHTN52loaJDa2lqHAgDo+TodQLt27ZLq6mqZM2eOXq6srJSAgAAJDw932C8qKkpvu5bc3FwJCwuzl9jY2M5WCQDgDQGkDrdNnjxZYmJiulSB7OxsffiutZSXl3fp+QAAPfhE1C+++EL++te/yttvv21fFx0drQ/LqV7Rlb0gNQtObbuWwMBAXQAA3qVTPSA1uaB///4yZcoU+7rExETx9/eXgoIC+7qSkhIpKyuTpKQk59QWAOC9PaCWlhYdQOnp6WKz/f/D1fjNvHnzJCsrSyIiIiQ0NFQWLlyow4cZcACALgeQOvSmejVq9tvVVq5cKb6+vvoEVDW7LTU1VdauXdvRlwAAeIEOB9D9998vlmW1uy0oKEjWrFmjCwAA18O14AAARhBAAAAjCCAAgBEEEADACAIIAGAEAQQAMIIAAgAYQQABAIwggAAARhBAAAAjCCAAgBEEEADAc36QDgDgWmFnmxyWQ8459hf86h23eyJ6QAAAIwggAIARBBAAwAgCCABgBJMQAMAN+V+6fNWy9Dj0gAAARhBAAAAjCCAAgBFuOwZUOi1AfIMDTFcDANBBLd+2iBy48X70gAAARhBAAAAjCCAAgBEEEADACB/LsixxI7W1tRIWFibfnBoiob3JRwDwNLUXWqTPbWelpqZGQkNDr7kfn/AAACMIIACAEQQQAMAIAggAYAQBBAAwggACALh/ADU3N0tOTo4MHjxYgoODZejQobJs2TK5cia3ur906VIZMGCA3iclJUVOnz7tiroDALwlgF5++WVZt26d/PGPf5TPP/9cL69YsUJef/11+z5qefXq1bJ+/Xo5dOiQ9OrVS1JTU6W+vt4V9QcAeMPVsD/66COZNm2aTJkyRS8PGjRI3nrrLTl8+LC997Nq1Sp59tln9X7Kli1bJCoqSnbt2iWzZs1yxd8AAOjpPaAJEyZIQUGBnDp1Si9/8skncuDAAZk8ebJeLi0tlcrKSn3YrZW6qsG4ceOkqKio3edsaGjQVz+4sgAAer4O9YCeeeYZHRDx8fHi5+enx4RefPFFmT17tt6uwkdRPZ4rqeXWbVfLzc2V559/vvN/AQCg5/eAduzYIVu3bpVt27bJxx9/LJs3b5bf//73+razsrOz9fWCWkt5eXmnnwsA0EN7QE899ZTuBbWO5YwaNUq++OIL3YtJT0+X6Ohovb6qqkrPgmullm+//fZ2nzMwMFAXAIB36VAP6NKlS+Lr6/gQdSiupaVF31fTs1UIqXGiVuqQnZoNl5SU5Kw6AwC8rQc0depUPeYTFxcnI0aMkL///e/y6quvyty5c/V2Hx8fWbx4sbzwwgsybNgwHUjqvKGYmBiZPn26q/4GAEBPDyB1vo8KlCeeeELOnz+vg+WXv/ylPvG01ZIlS6Surk7mz58v1dXVkpycLHv27JGgoCBX1B8A4KH4QToAgFPxg3QAALdGAAEAjCCAAABGEEAAACMIIACAEQQQAMAIAggA4P4nonaH1tOSai/+3+V9AACepfXz+0anmbpdAF24cEHfDhzzL9NVAQB08fNcXVjAY66EoC5sWlFRIb1799aVj42N1T/RcL2zadH5q07Qvq5D+7oW7eu+7atiRX1+q8u1XX0Ba7fuAanK3nrrrfaLmyrqj+cN5jq0r2vRvq5F+7pn+16v59OKSQgAACMIIACAEW4dQOqXUp977jl+MdVFaF/Xon1di/b1/PZ1u0kIAADv4NY9IABAz0UAAQCMIIAAAEYQQAAAIwggAIARbhtAa9askUGDBklQUJCMGzdODh8+bLpKHik3N1fuvPNOfWmj/v37y/Tp06WkpMRhn/r6esnIyJDIyEgJCQmRGTNmSFVVlbE6e6rly5frq3csXrzYvo627bovv/xSHnroId2GwcHBMmrUKDl69Kh9u5rIu3TpUhkwYIDenpKSIqdPnzZaZ0/R3NwsOTk5MnjwYN12Q4cOlWXLljlcRNSl7Wu5oe3bt1sBAQHWn//8Z+uzzz6zHn30USs8PNyqqqoyXTWPk5qaauXl5VnHjx+3jh07Zv34xz+24uLirIsXL9r3eeyxx6zY2FiroKDAOnr0qDV+/HhrwoQJRuvtaQ4fPmwNGjTIGj16tLVo0SL7etq2a/773/9aAwcOtObMmWMdOnTIOnv2rPXBBx9YZ86cse+zfPlyKywszNq1a5f1ySefWD/5yU+swYMHW99++63RunuCF1980YqMjLTee+89q7S01MrPz7dCQkKs1157rVva1y0D6K677rIyMjLsy83NzVZMTIyVm5trtF49wfnz59VXG6uwsFAvV1dXW/7+/vqN1+rzzz/X+xQVFRmsqee4cOGCNWzYMGvv3r3Wvffeaw8g2rbrnn76aSs5Ofma21taWqzo6GjrlVdesa9T7R4YGGi99dZb3VRLzzVlyhRr7ty5DuvS0tKs2bNnd0v7ut0huMbGRikuLtbdvCsvUKqWi4qKjNatJ6ipqdG3ERER+la1dVNTk0N7x8fHS1xcHO19k9QhtilTpji0oULbdt27774rY8eOlZkzZ+pDyHfccYds2LDBvr20tFQqKysd2lhdBFMdtqeNb2zChAlSUFAgp06d0suffPKJHDhwQCZPntwt7et2V8P++uuv9XHJqKgoh/Vq+eTJk8bq1ROon7pQ4xN33323jBw5Uq9Tb66AgAAJDw9v095qG65v+/bt8vHHH8uRI0fabKNtu+7s2bOybt06ycrKkl//+te6nZ988kndrunp6fZ2bO/zgja+sWeeeUb/7IL6YuTn56c/e1988UWZPXu23u7q9nW7AIJrv6kfP35cf8NB16nfSVm0aJHs3btXT5aBa740qR7QSy+9pJdVD0i9h9evX68DCF2zY8cO2bp1q2zbtk1GjBghx44d019S1e/4dEf7ut0huL59++okvnqmkFqOjo42Vi9Pt2DBAnnvvffkb3/7m/33lhTVpuqwZ3V1tcP+tPeNqUNs58+flzFjxojNZtOlsLBQVq9ere+rb4m0bdeomVfDhw93WJeQkCBlZWX6fms78nnROU899ZTuBc2aNUvPLvzFL34hmZmZevZsd7Sv2wWQ6lonJibq45JXfgtSy0lJSUbr5onURBMVPjt37pR9+/bp6ZZXUm3t7+/v0N5qmrb6D057X9+kSZPk008/1d8aW4v6tq4OX7Tep227Rh0uvvq0ATVeMXDgQH1fvZ/VB+GVbawOKR06dIg2vgmXLl1q84ulqgOgPnO7pX0tN52GrWZZbNq0yTpx4oQ1f/58PQ27srLSdNU8zuOPP66nUH744YfWuXPn7OXSpUsOU4XV1Ox9+/bpqcJJSUm6oOOunAWn0LZdn95us9n0dOHTp09bW7dutW655RbrzTffdJgmrD4f3nnnHesf//iHNW3aNKZh36T09HTrO9/5jn0a9ttvv2317dvXWrJkSbe0r1sGkPL666/r/7jqfCA1LfvgwYOmq+SR1HeM9oo6N6iVeiM98cQTVp8+ffR/7p/+9Kc6pND1AKJtu2737t3WyJEj9ZfS+Ph464033nDYrqYK5+TkWFFRUXqfSZMmWSUlJcbq60lqa2v1+1V91gYFBVlDhgyxfvOb31gNDQ3d0r78HhAAwAi3GwMCAHgHAggAYAQBBAAwggACABhBAAEAjCCAAABGEEAAACMIIACAEQQQAMAIAggAYAQBBAAQE/4XHXl3MfWvq5gAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# general imports\n",
        "import torch\n",
        "import yaml\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "\n",
        "# training imports\n",
        "from stable_baselines3.common.env_util import make_atari_env\n",
        "from stable_baselines3.common.vec_env import VecFrameStack, VecTransposeImage\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# IMPORTANT - REGISTER THE ENVIRONMENTS\n",
        "import gymnasium as gym\n",
        "import ale_py \n",
        "gym.register_envs(ale_py)\n",
        "\n",
        "# Load config\n",
        "_config_path = \"./configs.yaml\"\n",
        "\n",
        "_config = {}\n",
        "with open(_config_path, \"r\") as f:\n",
        "    _config = yaml.safe_load(f) or {}\n",
        "        \n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  # ignore tensorflow warnings about CPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "seed = None\n",
        "if seed is not None:    \n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "\n",
        "env = _config.get(\"ENVS\", \"PongNoFrameskip-v4\")[0]\n",
        "with open(f'environment_configs/{env}.yaml', 'r') as file:\n",
        "        environment_configuration = yaml.safe_load(file)[\"config\"]\n",
        "\n",
        "\n",
        "environment_configuration[\"f_ext_kwargs\"][\"device\"] = device  #do not comment this, it is the parameter passed to the feature extractor\n",
        "environment_configuration[\"game\"] = env\n",
        "\n",
        "\n",
        "\n",
        "vec_envs = make_atari_env(env, n_envs=environment_configuration[\"n_envs\"], seed=seed)\n",
        "vec_envs = VecFrameStack(vec_envs, n_stack=environment_configuration[\"n_stacks\"])\n",
        "vec_envs = VecTransposeImage(vec_envs)\n",
        "\n",
        "# execute some steps with random moves\n",
        "obs = vec_envs.reset()\n",
        "\n",
        "for i in range(10):\n",
        "    action = [vec_envs.action_space.sample() for _ in range(environment_configuration[\"n_envs\"])]\n",
        "    obs, rewards, dones, info = vec_envs.step(action)\n",
        "\n",
        "# obs[0] has shape (4, 84, 84) because there are 4 stacked environments, take the first\n",
        "observation = obs[0][-1]\n",
        "plt.imshow(observation)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4, 84, 84)\n",
            "\n",
            "Testing skill input adapters:\n",
            "\n",
            "Skill: state_rep_uns\n",
            "Input shape: torch.Size([1, 4, 84, 84])\n",
            "After adapter: torch.Size([1, 1, 160, 210])\n",
            "\n",
            "Skill: obj_key_enc\n",
            "Input shape: torch.Size([1, 4, 84, 84])\n",
            "After adapter: torch.Size([1, 1, 84, 84])\n",
            "\n",
            "Skill: obj_key_key\n",
            "Input shape: torch.Size([1, 4, 84, 84])\n",
            "After adapter: torch.Size([1, 1, 84, 84])\n",
            "\n",
            "Skill: vid_obj_seg\n",
            "Input shape: torch.Size([1, 4, 84, 84])\n",
            "After adapter: torch.Size([1, 2, 84, 84])\n"
          ]
        }
      ],
      "source": [
        "from skills.autoencoder import Autoencoder\n",
        "from skills.unsupervised_state_representation import UnsupervisedStateRepresentationModel\n",
        "from skills.video_object_keypoints import Transporter\n",
        "from skills.video_object_segmentation import VideoObjectSegmentationModel\n",
        "from utils.feature_extractors import WeightSharingAttentionExtractor\n",
        "\n",
        "# init skills\n",
        "autoencoder = Autoencoder(channels=1).to(device)\n",
        "\n",
        "# observation has shape (32, 4, 84, 84), \n",
        "# observation = obs[0][0][None, :, :] # (1, 84, 84)\n",
        "\n",
        "print(obs[0].shape) # (4, 84, 84)\n",
        "\n",
        "\n",
        "usr = UnsupervisedStateRepresentationModel(observation=obs[0], device=device)\n",
        "vok = Transporter().to(device)\n",
        "vos = VideoObjectSegmentationModel(device=device)\n",
        "\n",
        "\n",
        "skills = [\n",
        "    usr.get_skill(device=device),\n",
        "    vok.get_skill(device=device, keynet_or_encoder=\"encoder\"),\n",
        "    vok.get_skill(device=device, keynet_or_encoder=\"keynet\"),\n",
        "    vos.get_skill(device=device)\n",
        "]\n",
        "\n",
        "# Test each skill's input adapter\n",
        "print(\"\\nTesting skill input adapters:\")\n",
        "test_obs = obs[:1]  # Take one sample from batch\n",
        "test_obs = torch.tensor(test_obs, dtype=torch.float32).to(device)\n",
        "\n",
        "for skill in skills:\n",
        "    print(f\"\\nSkill: {skill.name}\")\n",
        "    print(f\"Input shape: {test_obs.shape}\")\n",
        "    adapted = skill.input_adapter(test_obs)\n",
        "    print(f\"After adapter: {adapted.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\giaco\\VSCode Projects\\medium-articles\\skill-based-agents\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "from stable_baselines3 import PPO\n",
        "from rl_zoo3.utils import linear_schedule\n",
        "\n",
        "\n",
        "f_ext_kwargs = environment_configuration[\"f_ext_kwargs\"]\n",
        "\n",
        "environment_configuration[\"f_ext_name\"] = \"wsharing_attention_ext\"\n",
        "environment_configuration[\"f_ext_class\"] = WeightSharingAttentionExtractor\n",
        "\n",
        "f_ext_kwargs[\"skills\"] = skills\n",
        "f_ext_kwargs[\"features_dim\"] = 256\n",
        "\n",
        "\n",
        "policy_kwargs = dict(\n",
        "    features_extractor_class=environment_configuration[\"f_ext_class\"],\n",
        "    features_extractor_kwargs=f_ext_kwargs,\n",
        "    net_arch={\n",
        "        \"pi\": environment_configuration[\"net_arch_pi\"],\n",
        "        \"vf\": environment_configuration[\"net_arch_vf\"],\n",
        "    },\n",
        "    # activation_fn=th.nn.ReLU,  # use ReLU in case of multiple layers for the policy learning network\n",
        ")\n",
        "\n",
        "logdir = \"./tensorboard_logs\"\n",
        "\n",
        "model = PPO(\n",
        "    \"CnnPolicy\",\n",
        "    vec_envs,\n",
        "    learning_rate=linear_schedule(environment_configuration[\"learning_rate\"]),\n",
        "    n_steps=128,\n",
        "    n_epochs=4,\n",
        "    batch_size=environment_configuration[\"batch_size\"],\n",
        "    clip_range=linear_schedule(environment_configuration[\"clip_range\"]),\n",
        "    normalize_advantage=environment_configuration[\"normalize\"],\n",
        "    ent_coef=environment_configuration[\"ent_coef\"],\n",
        "    vf_coef=environment_configuration[\"vf_coef\"],\n",
        "    policy_kwargs=policy_kwargs,\n",
        "    verbose=1,\n",
        "    device=device,\n",
        "    tensorboard_log=logdir,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logging to ./tensorboard_logs\\test_run_1_wsa_2\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 60   |\n",
            "|    iterations      | 1    |\n",
            "|    time_elapsed    | 16   |\n",
            "|    total_timesteps | 1024 |\n",
            "-----------------------------\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     12\u001b[39m eval_callback = EvalCallback(\n\u001b[32m     13\u001b[39m     eval_env,\n\u001b[32m     14\u001b[39m     n_eval_episodes=\u001b[32m100\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m     verbose=\u001b[32m0\u001b[39m,\n\u001b[32m     19\u001b[39m )\n\u001b[32m     21\u001b[39m callbacks = [eval_callback]\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giaco\\VSCode Projects\\medium-articles\\skill-based-agents\\.venv\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:311\u001b[39m, in \u001b[36mPPO.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    303\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[32m    304\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    309\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    310\u001b[39m ) -> SelfPPO:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giaco\\VSCode Projects\\medium-articles\\skill-based-agents\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:337\u001b[39m, in \u001b[36mOnPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    334\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    335\u001b[39m         \u001b[38;5;28mself\u001b[39m.dump_logs(iteration)\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m callback.on_training_end()\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giaco\\VSCode Projects\\medium-articles\\skill-based-agents\\.venv\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:213\u001b[39m, in \u001b[36mPPO.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.action_space, spaces.Discrete):\n\u001b[32m    210\u001b[39m     \u001b[38;5;66;03m# Convert discrete action from float to long\u001b[39;00m\n\u001b[32m    211\u001b[39m     actions = rollout_data.actions.long().flatten()\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m values, log_prob, entropy = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrollout_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    214\u001b[39m values = values.flatten()\n\u001b[32m    215\u001b[39m \u001b[38;5;66;03m# Normalize advantage\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giaco\\VSCode Projects\\medium-articles\\skill-based-agents\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:730\u001b[39m, in \u001b[36mActorCriticPolicy.evaluate_actions\u001b[39m\u001b[34m(self, obs, actions)\u001b[39m\n\u001b[32m    720\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    721\u001b[39m \u001b[33;03mEvaluate actions according to the current policy,\u001b[39;00m\n\u001b[32m    722\u001b[39m \u001b[33;03mgiven the observations.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    727\u001b[39m \u001b[33;03m    and entropy of the action distribution.\u001b[39;00m\n\u001b[32m    728\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    729\u001b[39m \u001b[38;5;66;03m# Preprocess the observation if needed\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m730\u001b[39m features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.share_features_extractor:\n\u001b[32m    732\u001b[39m     latent_pi, latent_vf = \u001b[38;5;28mself\u001b[39m.mlp_extractor(features)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giaco\\VSCode Projects\\medium-articles\\skill-based-agents\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:672\u001b[39m, in \u001b[36mActorCriticPolicy.extract_features\u001b[39m\u001b[34m(self, obs, features_extractor)\u001b[39m\n\u001b[32m    663\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    664\u001b[39m \u001b[33;03mPreprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[32m    665\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    669\u001b[39m \u001b[33;03m    features for the actor and the features for the critic.\u001b[39;00m\n\u001b[32m    670\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    671\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.share_features_extractor:\n\u001b[32m--> \u001b[39m\u001b[32m672\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures_extractor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures_extractor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures_extractor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    674\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m features_extractor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giaco\\VSCode Projects\\medium-articles\\skill-based-agents\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:131\u001b[39m, in \u001b[36mBaseModel.extract_features\u001b[39m\u001b[34m(self, obs, features_extractor)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[33;03mPreprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[32m    125\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    128\u001b[39m \u001b[33;03m:return: The extracted features\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    130\u001b[39m preprocessed_obs = preprocess_obs(obs, \u001b[38;5;28mself\u001b[39m.observation_space, normalize_images=\u001b[38;5;28mself\u001b[39m.normalize_images)\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfeatures_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed_obs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giaco\\VSCode Projects\\medium-articles\\skill-based-agents\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giaco\\VSCode Projects\\medium-articles\\skill-based-agents\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giaco\\VSCode Projects\\medium-articles\\skill-based-agents\\utils\\feature_extractors.py:177\u001b[39m, in \u001b[36mWeightSharingAttentionExtractor.forward\u001b[39m\u001b[34m(self, observations)\u001b[39m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, observations: torch.Tensor) -> torch.Tensor:\n\u001b[32m    173\u001b[39m     \u001b[38;5;66;03m#print(\"forward observation shape\", observations.shape)\u001b[39;00m\n\u001b[32m    174\u001b[39m     \u001b[38;5;66;03m# -------------- saving stats -------------- #\u001b[39;00m\n\u001b[32m    175\u001b[39m     weights = []\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpreprocess_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# this will populate self.skills_embeddings\u001b[39;00m\n\u001b[32m    179\u001b[39m     encoded_frame = \u001b[38;5;28mself\u001b[39m.get_last_frame_embedding_for_context(observations)\n\u001b[32m    180\u001b[39m     encoded_frame = \u001b[38;5;28mself\u001b[39m.encoder_lin_layer(encoded_frame)  \u001b[38;5;66;03m# query\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giaco\\VSCode Projects\\medium-articles\\skill-based-agents\\utils\\feature_extractors.py:74\u001b[39m, in \u001b[36mFeaturesExtractor.preprocess_input\u001b[39m\u001b[34m(self, observations)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     73\u001b[39m     so = skill.input_adapter(observations)\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     so = \u001b[43mskill\u001b[49m\u001b[43m.\u001b[49m\u001b[43mskill_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mskill\u001b[49m\u001b[43m.\u001b[49m\u001b[43mskill_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mso\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# can return linear or spatial embeddings\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m skill.name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.adapters:\n\u001b[32m     77\u001b[39m     adapter = \u001b[38;5;28mself\u001b[39m.adapters[skill.name]\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giaco\\VSCode Projects\\medium-articles\\skill-based-agents\\skills\\video_object_segmentation.py:269\u001b[39m, in \u001b[36mVideoObjectSegmentationModel.vos_output_masks\u001b[39m\u001b[34m(self, model, x)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvos_output_masks\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, x):\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giaco\\VSCode Projects\\medium-articles\\skill-based-agents\\skills\\video_object_segmentation.py:78\u001b[39m, in \u001b[36mVideoObjectSegmentationModel.compute_masks\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# [ BS x K x H x W ]\u001b[39;00m\n\u001b[32m     77\u001b[39m m = \u001b[38;5;28mself\u001b[39m.conv_m3(m)\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m m = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giaco\\VSCode Projects\\medium-articles\\skill-based-agents\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giaco\\VSCode Projects\\medium-articles\\skill-based-agents\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giaco\\VSCode Projects\\medium-articles\\skill-based-agents\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:359\u001b[39m, in \u001b[36mSigmoid.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    355\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    356\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    357\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    358\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold, StopTrainingOnNoModelImprovement\n",
        "\n",
        "\n",
        "eval_env = make_atari_env(env, n_envs=environment_configuration[\"n_envs\"])\n",
        "eval_env = VecFrameStack(eval_env, n_stack=environment_configuration[\"n_stacks\"])\n",
        "eval_env = VecTransposeImage(eval_env)\n",
        "\n",
        "run_id = \"test_run_1_wsa\"\n",
        "eval_logs = f\"eval_logs/{env}/{run_id}\"\n",
        "os.makedirs(eval_logs, exist_ok=True)\n",
        "\n",
        "eval_callback = EvalCallback(\n",
        "    eval_env,\n",
        "    n_eval_episodes=100,\n",
        "    best_model_save_path=f\"./agents/{run_id}\",\n",
        "    log_path=eval_logs,\n",
        "    eval_freq=5000 * environment_configuration[\"n_envs\"],\n",
        "    verbose=0,\n",
        ")\n",
        "\n",
        "callbacks = [eval_callback]\n",
        "\n",
        "model.learn(10000, callback=callbacks, tb_log_name=run_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMtMYo6f1slK1K86xPI/xkL",
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "skill-based-agents (3.12.12)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
